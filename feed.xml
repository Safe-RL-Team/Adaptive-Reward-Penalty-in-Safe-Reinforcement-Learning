<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://iclr-blogposts.github.io/staging/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/staging/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-02-11T07:52:32+00:00</updated><id>https://iclr-blogposts.github.io/staging/feed.xml</id><title type="html">ICLR Blogposts 2023 (staging)</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Adaptive Reward Penalty in Safe Reinforcement Learning</title><link href="https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/" rel="alternate" type="text/html" title="Adaptive Reward Penalty in Safe Reinforcement Learning"/><published>2023-01-31T00:00:00+00:00</published><updated>2023-01-31T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/"><![CDATA[<h1 id="introduction-to-safe-reinforcement-learning">Introduction to Safe Reinforcement Learning</h1> <p>Safe RL can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or the deployment processes <d-cite key="garcia_comprehensive_2015"></d-cite>.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://iclr-blogposts.github.io/staging/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/RL_boat_racing.mp4" style="width:500px" type="video/mp4"> </video> <figcaption> Open AIs CoastRunners agent from their blog post <a href="https://openai.com/blog/faulty-reward-functions">"Faulty Reward Functions in the Wild"</a> in Dec 2016.</figcaption> </center> <p>Defining a reward function is crucial in <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/">Reinforcement Learning</a> for solving many problems of interest in AI. It is often based on the designers’ intuition of the goal of the system. In the above example of CoastRunners, the goal is to reach the finish line and collect points along the way. Whilst selecting the in-game score the player earned as a reflection of the informal goal of finishing the race is a reasonable reward function, it allows for dangerous and harmful behavior, as visible in the video above. The agent can drive off the track, crash into other boats, and catch fire and still win the game whilst achieving a score on average 20 percent higher than that achieved by human players.</p> <p>How can we prevent the agents from violating safety constraints (e.g., crashing into other boats)? Recent studies have started to address the problem of safe reinforcement learning from various perspectives, ICLR works including, but not limited to:</p> <ul> <li><a href="https://openreview.net/pdf?id=HJgEMpVFwB">Adversarial Policies: Attacking Deep Reinforcement Learning</a>, Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, <strong>ICLR 2020</strong></li> <li><a href="https://arxiv.org/pdf/2201.09802.pdf">Constrained Policy Optimization via Bayesian World Models</a>, Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, <strong>ICLR 2022</strong></li> <li><a href="https://openreview.net/pdf?id=TBIzh9b5eaz">Risk-averse Offline Reinforcement Learning</a>, Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, <strong>ICLR 2021</strong></li> <li><a href="https://openreview.net/pdf?id=S1vuO-bCW">Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>, Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, <strong>ICLR 2018</strong></li> <li><a href="https://openreview.net/pdf?id=iaO86DUuKi">Conservative Safety Critics for Exploration</a>, Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, <strong>ICLR 2021</strong></li> <li><a href="https://openreview.net/pdf?id=TQt98Ya7UMP">Balancing Constraints and Rewards with Meta-gradient D4PG</a>, Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, <strong>ICLR 2021</strong></li> </ul> <p>We chose to illustrate the method of Reward Constrained Policy Optimization (RCPO) <d-cite key="Tessler2018RCPO"></d-cite> in this blog post because it is a simple yet effective method of introducing the ideas of safe RL. By providing a high-level constraint, the agent learns to respect it and achieve the perfect balance between meeting that constraint and maximizing the reward. Moreover, this removes the need to manually extend and tune the reward function since it is adaptively shaped during the learning!</p> <h1 id="a-formalism-for-safe-reinforcement-learning-constrained-mdps">A Formalism for Safe Reinforcement Learning: Constrained MDPs</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration-1400.webp"/> <img src="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/CMDP_Illustration.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of a Constrained Markov Decision Process (MDP) adapted from <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview">Lilian Weng</a>. <br/> Based on an observation (also called state) from the environment, the agent selects an action. This action is executed in an environment resulting in a new state and a reward that evaluates the action. Given the new state, the feedback loop repeats. </div> <p>In Reinforcement Learning, the world is modeled as a Markov Decision Process (MDP) and the goal is to select a policy \(\pi\) which maximizes an expected cumulative reward \(J^π\).</p> <p>\(J^π\) can be taken to be the infinite horizon discounted total return as</p> \[J^\pi = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]\] <p>where \(\gamma\) is the discount factor, and \(r(s_t,a_t)\) is the reward function.</p> <p>However, the agents must obey safety constraints in many real-world applications while achieving the goal. We can introduce a constraint objective analogous to the reward objective. This objective is typically defined as the expected constraint value over N time steps \(J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]\). The method of aggregating individual constraints over time can vary, e.g., using the average or the maximum constraint value over N time steps or even a discounted sum.</p> <p>In the example of the robot, the aim could be to prolong the motor life of the various robots while still enabling them to perform the task at hand. Thus we constrain the robot motors from using high torque values. Here, constraint C is defined as the average torque the agent has applied to each motor, and the penalty \(c(s, a)\) becomes the average amount of torque the agent decided to use at each time step.</p> <p>We limit the allowable amount of torque applied to \(\alpha\). <br/> The constrained MDP for our safe reinforcement learning problem is:</p> \[\max_{\pi \in \Pi} J^\pi_R \text{ s.t. } J^\pi_C \leq \alpha\] <h1 id="constrained-policy-optimization">Constrained Policy Optimization</h1> <p>Constrained objectives are often solved using the Lagrange relaxation technique. With parameterized approaches such as Neural Networks, the objective is then to find the networks parameters \(\theta\) that maximize \(J^\pi_R\) subject to the constraint \(J^\pi_C \leq \alpha\) given the Lagrangian multiplier \(\lambda\):</p> \[\min_{\lambda}\max_{\theta} [J^{\pi_\theta}_R - \lambda (J^{\pi_\theta}_C - \alpha)]\] <p>We now have our new global objective function that is subject to optimization!</p> <h3 id="what-exactly-does-the-lagrangian-do">What exactly does the Lagrangian do?</h3> <p>Intuitively, the Lagrangian multiplier \(\lambda\) determines how much weight is put onto the constraint. If \(\lambda\) is set to 0, the constraint is ignored, and the objective becomes the reward objective \(J^\pi_R\). If \(\lambda\) is set very high, the constraint is enforced very strictly, and the global objective function reduces to the constraint objective \(J^π_C\). Let’s look at a simple example to <strong>demonstrate the effect of the Lagrangian multiplier \(\lambda\)</strong>. We’ll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was kept upright.</p> <p>We can now add an example constraint to the environment. Let’s say we want to keep the cart in the left quarter of the x-axis. We, therefore, define the constraint value as the x-position of the cart and the upper bound \(\alpha\) as -2.</p> <p>Let’s see how with different lambda values, the constraint is enforced.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://iclr-blogposts.github.io/staging/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/cart_pole_lambda.mp4" style="width:500px" type="video/mp4"> </video> <figcaption> The green area represents the "safe zone", where the x-position is smaller than -2, and the red area is the "unsafe zone". <br/> The lower the lambda, the more the constraint is ignored. The higher the lambda, the more the constraint is enforced, and the main reward objective is ignored. At λ = 1,000,000 the cart shoots to the right to tilt the pole to the left but does so ignoring the following balancing act, which is observable at λ ∈ {10, 100}.</figcaption> </center> <p>Tuning the \(\lambda\) through <a href="https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html">reward shaping</a> is no easy feat. The Lagrangian is a scaling factor, i.e., if the constraint values are inherently larger than the reward values, we will need a substantially lower \(\lambda\) than when the constraint values are significantly smaller than the possible reward values. That means that the range of good lambda values is large and differs with every environment.</p> <h3 id="how-can-we-learn-an-optimal-lagrangian">How can we learn an optimal Lagrangian?</h3> <p>Luckily, it is possible to <strong>view the Lagrangian as a learnable parameter</strong> and update it through gradient descent since the globally constrained optimization objective \(J^{\pi_{\theta}}\) is differentiable. In short, we can simply use the derivative of the objective function w.r.t \(\lambda\) and update the Lagrangian.</p> \[\frac{\partial J^{\pi_{\theta}}}{\partial \lambda} = -(J^{\pi_{\theta}}_C - \alpha)\] \[\lambda \gets max(\lambda - lr_{\lambda}(-(\mathbb{E}^{\pi_\theta}_{s\sim\mu} \left[C\right] - \alpha)), 0)\] <p>Hereby \(lr_{\lambda}\) is the learning rate for the Lagrangian multiplier. The max function ensures that the Lagrangian multiplier is always positive.</p> <h1 id="reward-constrained-policy-optimization">Reward Constrained Policy Optimization</h1> <p>Actor-Critic based approaches such as <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">PPO</a> <d-cite key="Schulman2017PPO"></d-cite> have empirically been shown to compete at the top of a plethora of quality benchmarks. In this class of algorithms, the actor learns a policy \(\pi\), whereas the critic learns the value function using temporal difference learning. Using the critic reduced the variance and <strong>enabled training using a finite number of samples</strong>.</p> <h3 id="how-to-integrate-the-constraint-into-the-actor-critic-approach">How to integrate the constraint into the Actor-Critic approach?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm-1400.webp"/> <img src="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/Algorithm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>If we look at the RCPO algorithm illustrated above, we can see that implementing the constraint into the Actor-Critic approach is done in a few lines of code. First, we need to collect the constraint during the policy rollout. Then we can integrate the constraint values (the guiding penalty) into the reward during the computation of the policy and value gradients, as demonstrated in line 7. <br/> This is done by formulating the constraint as the infinite horizon discounted total cost, similar to the usual returns of an MDP.</p> \[J^\pi_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]\] <p>Now we can simply include the guiding penalty to the reward function via the Lagrange multiplier to arrive at the penalized reward function:</p> \[\hat{r} = r(s,a) - \lambda c(s,a)\] <p>Finally, we can compute the gradient of the Lagrangian in line 11 and update \(\lambda\) in line 14 as discussed in the previous section and repeat the whole process for \(K\) iterations.</p> <h1 id="implementation">Implementation</h1> <p>To facilitate reproducibility, we integrated RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> PPO implementation.</p> <h3 id="integrating-the-guiding-penalty">Integrating the guiding penalty</h3> <p>For the computation of returns with PPO, we use the Temporal Difference Error (TD estimate) and the Generalized Advantage Estimation (GAE) advantage. <br/> To integrate the constraint into the reward function, we need to add the Lagrangian-scaled constraint value to the reward, as discussed in the RCPO section. This is done when computing the TD error estimate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_returns_and_advantages</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">last_values</span><span class="p">:</span> <span class="n">th</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dones</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>
<span class="p">)</span>
    <span class="c1"># ...
</span>    <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
        <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">constraints</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
        <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_values</span> <span class="o">*</span> <span class="n">next_non_terminal</span>
        <span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="n">step</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># ...
</span></code></pre></div></div> <blockquote> <p>The discussed integration of the constraint into the reward function is implemented into the computation of the advantages and returns. When the lambda parameter is set to 0, the constraint is ignored and the reward function is the same as in the original PPO implementation.</p> </blockquote> <p>Additionally, it was necessary to extend the rollout buffer to collect the constraint values at each time step. To receive the constraint values, we customized the gym environments to return those in the info dictionary.</p> <h3 id="updating-the-lagrangian-multiplier">Updating the Lagrangian multiplier</h3> <p>Due to the fact that PPO (1) collects multiple episodes until the rollout buffers are full and (2) supports vectorized environments, the logic for collecting and aggregating the constraint values across the episodes and parallel environments is a bit more complex. <br/> Nevertheless, we have chosen the aggregation method to be the average over all time steps in one complete episode and across all those episodes themselves, i.e., episodes that have reached a terminal state.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># lambda &lt;- lambda - lr_lambda * -(C - alpha) = lambda + lr_lambda * (C - alpha)
</span><span class="n">d_constraint_lambda</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">C</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">constraint_lambda</span>
<span class="bp">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">+=</span> <span class="p">(</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">lr_constraint_lambda</span> <span class="o">*</span> <span class="n">d_constraint_lambda</span>
<span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">.</span><span class="n">rollout_buffer</span><span class="p">.</span><span class="n">constraint_lambda</span><span class="p">,</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div> <blockquote> <p>After aggregating the constraint values across the episodes and parallel environments into self.C, the Lagrangian is updated using gradient descent. The max function is used to ensure that the Lagrangian multiplier is always positive.</p> </blockquote> <h1 id="experiments">Experiments</h1> <p>As a proof-of-the-principle experiment, we reproduced the results of the <a href="https://gymnasium.farama.org/environments/mujoco/">OpenAI HalfCheetah environment</a> from Tessler C. et al.<d-cite key="Tessler2018RCPO"></d-cite>.</p> <p>The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the same as in the original paper and let the agents train for \(1,000,000\) time steps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints-1400.webp"/> <img src="/staging/assets/img/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/experiments_results_smooth_constraints.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Rewards and average torque of the experiments on the HalfCheetah environment. The x-axis represents the time steps and the maximum torque constraint is illustrated by the dashed line. </div> <p>The results demonstrate that the RCPPO trained an agent that successfully walked forward while respecting the safety constraint. We achieved comparable results to the original experiments in the paper. <br/> Interestingly, low \(\lambda\) values seem to be less stable than higher \(\lambda\) values. The guiding penalty appears to enforce the constraint and improve the learning process overall. It limits the amount of torque the agent is allowed to apply, hinders the exploration of unsafe and poor-performing local minima and guides the policy to a safe and more optimal solution. <br/> Nevertheless, the poor performance of the unconstrained agents may be due to the neural network architecture being relatively small (i.e., 2 layers of 64 hidden units).</p> <h3 id="qualitative-observations">Qualitative observations</h3> <p>Finally, let’s see how our HalfCheetah agents walk under the attempt to enforced constraint. To do so, we have recorded videos of the agents walking forward with different \(\lambda\) values. The results can be seen below.</p> <center> <video autoplay="" muted="" loop="" controls="" src="https://iclr-blogposts.github.io/staging/assets/video/2023-01-31-Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/HalfCheetah_Experiments.mp4" style="width:500px" type="video/mp4"> </video> <figcaption>Visualization of the HalfCheetah agents learned through RCPPO and with different selected Lagrangian multipliers.</figcaption> </center> <p>We can again observe that the lower the lambda is, the more the constraint is ignored and the higher the lambda, the more the constraint is enforced and the main reward objective is ignored.<br/> At λ ∈ {10, 100}, the robot applies 0 torque to ultimately oblige to the constraint ignoring the main reward objective to walk forward, which is observable at λ ∈ {RCPPO, 0, 0.00001}. With λ ∈ {0, 0.00001} the robot can walk forward, but it is visible that it moves its legs much quicker and more aggressively than the RCPPO agent. Furthermore, the RCPPO agent walks perfectly, whilst the other (moving) agents tumble over their own hecktick steps.</p> <h1 id="discussion">Discussion</h1> <h3 id="theoretical-assumptions-vs-empirical-results">Theoretical assumptions vs. empirical results</h3> <p>We had to select higher values for the Lagrangian multiplier than were used in the original paper. In the paper, a \(\lambda\) value of 0.1 is already very high as it leads to a reward of \(-0.4\) and torque of \(0.1387\), whereas in our case a \(\lambda\) value of \(1.0\) leads to a reward of about \(1 500\) with an average torque of \(0.39\). <br/> This affected the reward shaping process but also meant we had to increase the Lagrangian’s respective learning rate so it can grow quicker when training it as a parameter. As a result, \(lr_{\lambda}\) becomes larger than \(lr_{\pi}\), which <strong>ignores one of the assumptions made in the paper</strong>, yet leads to coherent results.</p> <p>A possible reason for the slower and weaker impact of the constraint could be attributed to the clipping of the trust region. This technique ensures that the policy does not change too much between updates and prevents it from landing in a bad local minimum that it can not escape. This is done by clipping the policy update to a specific range. Therefore, even with “high” values of lambda w.r.t. the original paper, the policy will not change significantly with each update to conform to the constraint.</p> <p>Not only did we have to select a higher learning rate for the Lagrangian, but we also did not include different learning rates for the policy and the value function, <strong>ignoring the three times scales approach</strong> proposed in the original paper. Additionally, in the original paper, the RCPPO algorithm updated their networks (actor and critic) after each episode. In our implementation, we need to fill the rollout buffer with potentially multiple episodes, thus reducing the frequency of network parameters and Lagrangian updates. Nevertheless, the PPO algorithm implements a parameter update loop of \(N\) epochs after each rollout, which to a degree, counteracts the discussed lower update frequency of all parameters.</p> <h3 id="conclusion">Conclusion</h3> <p>The results of the experiments show that the RCPO approach can learn a policy that can optimize the main reward objective while respecting the constraint.</p> <p>Safe Reinforcement Learning is a critical area of research in the field of artificial intelligence, as it has the potential to shape the future of autonomous systems in a multitude of domains, ranging from robotics to finance. <br/> The more complex systems become, the more difficult it is to ensure safety requirements, especially through simple reward shaping. An approach such as RCPO can ensure that the safety constraints are respected while enforcing them by only providing the constraint itself.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In this blog, we dive into the ICLR 2019 paper Reward Constrained Policy Optimization (RCPO) by Tessler et al. and highlight the importance of adaptive reward shaping in safe reinforcement learning. We reproduce the paper's experimental results by implementing RCPO into Proximal Policy Optimization (PPO). This blog aims to provide researchers and practitioners with (1) a better understanding of safe reinforcement learning in terms of constrained optimization and (2) how penalized reward functions can be effectively used to train a robust policy.]]></summary></entry><entry><title type="html">Underfitting and Regularization: Finding the Right Balance</title><link href="https://iclr-blogposts.github.io/staging/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/" rel="alternate" type="text/html" title="Underfitting and Regularization: Finding the Right Balance"/><published>2023-01-15T00:00:00+00:00</published><updated>2023-01-15T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2023/Underfitting-and-Regularization-Finding-the-Right-Balance/"><![CDATA[<h2 id="goal-of-this-blog-post">Goal of this blog post</h2> <p>Network Augmentation aka NetAug <d-cite key="DBLP:conf/iclr/CaiG0022"></d-cite> caters to training small neural network architectures like MobileNetV2-Tiny for the best top-k percent accuracy. The paper argues that training small neural networks technically differs from that of large neural networks because the former is prone to underfitting. NetAug is contrary to traditional methods like dropout<d-cite key="10.5555/2627435.2670313"></d-cite>, network pruning<d-cite key="9043731"></d-cite>, quantization<d-cite key="jacob2018quantization"></d-cite>, data augmentation<d-cite key="https://doi.org/10.48550/arxiv.1712.04621"></d-cite> and other regularization techniques<d-cite key="10.1007/s10462-019-09784-7"></d-cite> NetAug can be viewed as reversed form of dropout, as we enlarge the target model during the training phase instead of shrinking it. In this blog post, we identify some pitfalls with NetAug and propose potential workarounds.</p> <h2 id="background">Background</h2> <p>NetAug solely focuses on improving the performance of the tiny neural networks during inference, whilst optimizing their memory footprint to deploy them on edge devices. Tiny neural networks are usually inclined to underfit. Hence, the traditional training paradigms will not work for these small models because they fundamentally tackle the problem of overfitting and not overcome the underfitting issue. Several techniques in the recent time like data augmentation, pruning, dropout, knowledge distillation have been proposed to improve the generalizability of neural networks.</p> <ol> <li> <dl> <dt><strong>Knowledge Distillation</strong><d-cite key="hinton2015distilling"></d-cite></dt> <dd>It is quite difficult to deploy and maintain an ensemble. However, previous research has shown that it is possible to learn a single cumbersome model that has the same performance as an ensemble. In most knowledge distillation methods there exist a large teacher model that transfers its knowledge as a learned mapping via training to a small student model with the teacher models output. There exists several techniques like self-distillation in which the teacher model trains itself continuously. Convectional KD methods try to optimise the objective function such that the loss function penalizes the difference between the student and teacher model.</dd> </dl> </li> <li> <dl> <dt><strong>Regularization</strong><d-cite key="10.1007/s10462-019-09784-7"></d-cite></dt> <dd>Regularization is used to prevent overfitting of any ML model by reducing the variance, penalizing the model coefficients and complexity of the model. Regularization techniques are mainly composed of data augmentation one of the most simplest and conveninet ways to expand the size of the dataset such that it prevents overfitting issues that occur with a relatively small dataset. Dropout is also popularly applied while training models, in which at every iteration incoming and outgoing connections between certain nodes are randomly dropped based on a particular probability and the remaining neural network is trained normally.</dd> </dl> </li> <li> <dl> <dt><strong>Tiny Deep learning</strong><d-cite key="lin2020mcunet"></d-cite>,<d-cite key="lin2021mcunetv2"></d-cite>,<d-cite key="lin2022ondevice"></d-cite></dt> <dd>Several challenges are paved while transitioning from conventional high end ML systems to low level clients, maintaining the accuracy of learning models, provide train-to-deploy facility in resource economical tiny edge devices, optimizing processing capacity. This method includes AutoML procedures for designing automatic techniques for architecturing apt neural netowrks for a given target hardware platform includes customised fast trained models,auto channel pruning methods and auto mixed precision quantization. The other approaches like AutoAugment methods automatically searches for improvised data augmentation within the network to prevent overfitting. There exists network slimming methods to reduce model size, decrease the run-time memory footprint and computing resource.</dd> </dl> </li> <li> <dl> <dt><strong>Network Augmentation</strong><d-cite key="DBLP:conf/iclr/CaiG0022"></d-cite></dt> <dd>This method was proposed to solve the problem of underfitting in tiny neural networks. This is done by augmenting the given model (referred to as base model) into a larger model and encourage it to work as a sub-model to get extra supervision in additoin to functioning independently. This will help in increasing the representation power of the base model because of the gradient flow from the larger model and it can be viewed equivalently as “<strong>reverse-dropout</strong>”.</dd> </dl> </li> <li> <dl> <dt><strong>Neural Architechture Search (NAS)</strong><d-cite key="https://doi.org/10.48550/arxiv.1808.05377"></d-cite></dt> <dd>This method was proposed to solve the problem of architecture optimization and weight optimization based on the underlying training data. NAS usually involves defining an architectural search space and then searching for the best architecture based on the performance on the validation set. Recent approaches include weight sharing, nested optimization and joint optimization during training. However, there are drawbacks in using these approaches because they are computationally expensive and suffer from coupling between architecture parameters and model weights. This will degrade the performance of the inherited weights.</dd> </dl> </li> </ol> <h2 id="formulation-of-netaug">Formulation of NetAug</h2> <p>The end goal of any ML model is to be able to minimize the loss function with the help of gradient descent. Since tiny neural network have a very small capacity the gradient descent is likely to get stuck in local minima. Instead of traditional regularization techniques which add noise to data and model, NetAug proposes a way to increase the capacity of the tiny model without changing its architecture for efficient deployment and inference on edge devices. This is done by augmenting the tiny model (referred to as base model) into a larger model and jointly training both the base model independently and also the augmented model so that the base model benefits from the extra supervision it receives from the augmented model. However, during inference only the base model is used.</p> <p>To speed-up the training, a single largest augmented model is constructed by augmenting the width of the each layer of the base model using an augmentation factor \(r\). After building the largest augmented model, we construct other augmented models by selecting a subset of channels from the largest augmented model. NetAug proposes a hyper-parameter \(s\), named diversity factor, to control the number of augmented model configurations. We set the augmented widths to be linearly spaced between \(w\) and \(r \times w\). For instance, with \(r = 3\) and \(s = 2\), the possible widths would be \([w, 2w, 3w]\).</p> <h2 id="pitfalls-in-netaug">Pitfalls in NetAug</h2> <ol> <li> <dl> <dt><strong id="training_aug_models">Training the Generated Augmented Models</strong></dt> <dd>NetAug randomly samples sub-models from the largest model by augmenting the width instead of depth, its highly important to ensure we speed up the training time and reduce the number of traning iterations for these generated sub-models thereby enhancing the training convergence and reaching optimization faster. We theoretically aim at aiding this by introducing a re-parametrisation technique during training that involves sharing and unsharing of weights to attain convergence much faster. </dd> </dl> </li> <li> <dl> <dt><strong id="naive_loss">Naive Loss Function</strong></dt> <dd>NetAug computes loss in a very trivial form, i.e, by simply performing a weighted sum over the loss of the base model with that from the respective sampled augmented models. However, it was mentioned in the paper that sampling more than one sub-models from the largest augmented model in each training step is resulting in degradation of the base model’s performance. This can be attributed to the fact that simple weighted sum of losses from the base supervision and auxiliary supervision is causing the auxiliary supervision to shadow the base model. We propose different mixing strategies to circumvent this problem. </dd> </dl> </li> <li> <dl> <dt><strong id="generating_aug_model">Generating the Largest Augmented Model</strong></dt> <dd>For a particular network, rather than tuning for just a single network hyperparameter (i.e., network, depth, width etc.), what if we instead tune all the closely relevant network hyperparameters for every augmented sub-model? To advocate this it’s sensible to compare the entire distribution of hyperparameter across the model. This can be tackled using NAS to find the best largest augmented model and then use it for auxiliary supervision of the base model.</dd> </dl> </li> </ol> <h2 id="introducing-netaug-with-relation-knowledge-distribution-rkd">Introducing NetAug with Relation Knowledge Distribution (RKD)</h2> <p>Knowledge distillation in learned models is constituted of:</p> <ol> <li> <dl> <dt><strong>Individual Knowledge Distillation</strong></dt> <dd>Outputs of individual samples represented by the teacher and student are matched.</dd> </dl> </li> <li> <dl> <dt><strong>Relational Knowledge Distillation</strong></dt> <dd>Relation among examples represented by the teacher and student are matched. RKD is a generalization of convectional knowledge distillation that combines with NetAug to boost the performance due to its complementarity with conventional KD, that aims at transferring structural knowledge using mutual relations of data examples in the teacher’s output presentation rather than individual output themselves. Contrary to conventional approaches called as Individual KD (IKD) that transfers individual outputs of the teacher model \(f_T(\cdot)\) to the student model \(f_S(\cdot)\) point-wise, RKD transfers relations of the outputs structure-wise and computes a relational potential \(\psi\) for every \(n\)-tuple of data instance and transfers the relevant information through the potential from the teacher to the student models. In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps are also used to train a student model.</dd> </dl> </li> </ol> <p>We specifically aim at training the teacher and student model in a online setting, in this online type of distillation training method both the teacher and the student model are trained together simultaneously.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd-1400.webp"/> <img src="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/rkd.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Relational Knowledge distillation can be expressed as</strong> -<br/> Given a teacher model \(\:T\:\) and a student model \(S\), we denote \(f_T(\cdot)\) and \(f_S(\cdot)\) as the functions of the teacher and the student, respectively, and \(\psi\) as a function extracting the relation, we have</p> \[\begin{equation} \mathcal{L}_{\text{RKD}} = \sum \limits_{\{x_1, \ldots, x_n\} \in \chi^N} l \big(\psi(t_1,\cdots,t_n), \: \psi(s_1, \cdots, s_n)\big) \end{equation}\] <p>where \(\mathcal{L}_{\text{RKD}}\) is the loss function, \(t_i = f_T(x_i)\) and \(s_i = f_S(x_i)\) and \(x_i \in \chi\) denotes the input data.</p> <h3 id="loss-functions-in-rkd">Loss Functions in RKD</h3> <ol> <li><strong>Distance-wise distillation loss (pair)</strong></li> </ol> <div class="small_img row mt-1"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D-1400.webp"/> <img src="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-D.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This method is known as RKD-D. It transfers relative distance between points on embedding space. Mathematically, \(\begin{equation}\psi_{D}(t_i, t_j) = \frac{1}{\mu}\big\| t_i - t_j\big\|_2\end{equation}\) where \(\psi_d(\cdot, \cdot)\) denotes distance wise potential function \(\begin{equation}\mu = \frac{1}{|\chi^2|}\sum\limits_{(x_i, x_j) \in \chi^2} \big\| t_i - t_j\big\|_2\end{equation}\) \(\begin{equation}\boxed{\mathcal{L}_{\text{RKD-D}} = \sum \limits_{(x_i, x_j) \in \chi^2} l_\delta \big(\psi_D(t_i, t_j), \psi_D(s_i, s_j)\big)}\end{equation}\) where \(l_{\delta}\) denotes the Huber Los</p> \[\begin{equation}l_\delta(x, y) = \begin{cases} \frac{1}{2} (x-y)^2\:\:\:\: \text{for } |x-y| \leq 1 \\ |x - y| - \frac{1}{2}\:\:\: \text{otherwise.} \end{cases}\end{equation}\] <ol> <li><strong>Angle-wise distillation loss (triplet)</strong></li> </ol> <div class="small_img row mt-1"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A-1400.webp"/> <img src="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/RKD-A.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This method is known as RKD-A. RKD-A transfers angle formed by three points on embedding space. Mathematically, \(\begin{equation}\psi_{A}(t_i, t_j, t_k) = \cos \angle t_it_jt_k = \langle \boldsymbol{e}^{ij}, \boldsymbol{e}{jk}\rangle\end{equation}\) where \(\psi_A(\cdot, \cdot, \cdot)\) denotes angle wise potential function \(\begin{equation}\boldsymbol{e}^{ij} = \frac{t_i - t_j}{\big\|t_i - t_j\big\|_2}, \: \boldsymbol{e}^{jk} = \frac{t_k - t_j}{\big\|t_k - t_j\big\|_2}\end{equation}\)</p> \[\begin{equation}\boxed{\mathcal{L}_{\text{RKD-A}} = \sum\limits_{(x_i, x_j, x_k) \in \chi^3} l_\delta \big(\psi_A(t_i, t_j, t_k), \psi_A(s_i, s_j, s_k)\big)}\end{equation}\] <h3 id="combining-rkd-with-netaug">Combining RKD with NetAug</h3> <p>We propose the following loss function to solve <a href="#naive_loss">naive loss problem of NetAug</a></p> \[\begin{equation}\mathcal{L}_{\text{aug}} = \underbrace{\mathcal{L}(W_t)}_{\text{base supervision}} \:+\: \underbrace{\alpha_1 \mathcal{L}([W_t, W_1]) + \cdots + \alpha_i \mathcal{L}([W_t, W_i]) + \cdots}_{\text{auxiliary supervision, working as a sub-model of augmented models}} \:+\: \lambda_{\text{KD}}\,\underbrace{\mathbf{\mathcal{L}_{\text{RKD}}}}_{\text{relational knowledge distillation}}\label{eqn:loss_func}\end{equation}\] <p>where \([W_t, W_i]\) represents an augmented model where \([W_t]\) represents the tiny neural network and \([W_i]\) contains weight of the sub-model sampled from the largest augmented model, \(\alpha\) is scaling hyper-parameter for combining loss from different augmented models and finally \(\lambda_{\text{KD}}\) is a tunable hyperparameter to balance RKD and NetAug.</p> <h3 id="netaug-training-with-rkd">NetAug Training with RKD</h3> <p>In NetAug, they train only one model for every epoch, training all the augmented models all once is not only computationally expensive but also impacts the performance. The proportion of the base supervision will decrease when we sample more augmented networks, which will make the training process biased toward augmented networks and shadows the base model.</p> <p>To further enhance the auxiliary supervision, we propose to use RKD in an online setting i.e., the largest augmented model will act as a teacher and the base model will act as a student. Both the teacher and the student are trained simultaneously.</p> <p>We train the both the base model and the augmented model via gradient descent based on the loss function \(\eqref{eqn:loss_func}\). The gradient update for the base model is then given by</p> \[\begin{equation}W^{n+1}_t = W^n_t - \eta \bigg(\frac{\partial \mathcal{L}(W^n_t)}{\partial W^n_t} + \alpha \frac{\partial \mathcal{L}([W^n_t, W^n_i])}{\partial W^n_t} + \lambda \frac{\partial \mathcal{L}_{\text{RKD}}([W^n_t, W^n_l])}{\partial W^n_t}\bigg)\end{equation}\] <p>Similar update equations can be obtained for the largest augmented model and the sub-models as well.</p> <h2 id="fasten-auxillary-model-training">Fasten Auxillary Model Training</h2> <p>We propose this method to solve the problem <a href="#training_aug_models">training the generated augmented models</a>. Based on <d-cite key="yang2021speeding"></d-cite> we propose to speed up the training process for the augmented sub models such that it can attain faster convergence and reduce the number of training iterations thereby obtain a better performance. In this technique, in the early phase of training, the neural network is trained with weights shared across all the layers of the model, to learn the commonly shared component across weights of different layers, and towards the later phase of training we un-share weights and continue training until convergence. Weight sharing for initial training steps will contrain the model complexity effectively. It brings the weights closer to the optimal value, which provides a better initialization for subsequent training steps and improved model generalization.</p> <p><strong>Mathematical Formulation</strong></p> <p>Denote the neural model as consisting of \(L\) stacked structurally similar modules as \(\mathcal{M} = \{\mathcal{M}_i\}, \, i=1,\cdots, L\) and \(w = \{w_i\}, \, i=1,\cdots, L\) denote the corresponding weights. These weights are re-parametrized as</p> \[\begin{equation}w_i = \frac{1}{\sqrt{L}}w_0 + \tilde{w}_i, \:\:\:\: i=1,\cdots,L\end{equation}\] <p>Here \(w_0\) represents the shared weights across all modules and is referred to as <strong>stem-direction</strong> and \(\tilde{w}_i\) represents the unshared weights across all modules and is referred to as <strong>branch-directions</strong>.</p> <p><strong>Training Strategy</strong></p> <p>Denote \(T\) as the number of training steps, \(\eta\) as the step size and \(\alpha \in (0, 1)\) is a tunable hyper-paramter indicating the fraction of weight sharing steps. Then we train \(\mathcal{M}\) as follows:</p> <ul> <li><strong>Sharing weights in early stage:</strong> For the first \(\tau = \alpha \cdot T\) steps, we update the shared weights \(w_0\) alone with gradient \(g_0\)</li> <li><strong>Unsharing weights in later stage:</strong> For the next \(t \geq \alpha \cdot T\), we update only the unshared weights \(\tilde{w}_i\) with gradient \(\tilde{g}_i\)</li> </ul> <p>The effective gradient updates for \(w_i\) can be found using chain rule as follows:</p> \[\begin{equation}g_0 = \frac{\partial \mathcal{L}}{\partial w_0} = \sum\limits_{i=1}^L \frac{\partial \mathcal{L}}{w_i}\,\frac{\partial w_i}{\partial w_0} = \frac{1}{\sqrt{L}}\sum\limits_{i=1}^L g_i \end{equation}\] \[\begin{equation}\tilde{g}_i = \frac{\partial \mathcal{L}}{\partial \tilde{w}_i} = \frac{\partial \mathcal{L}}{\partial w_i}\,\frac{\partial w_i}{\partial \tilde{w}_i} = g_i\end{equation}\] <p>where \(g_i\) denotes the gradients of \(w_i\) and \(\mathcal{L}\) denotes the loss function.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE-1400.webp"/> <img src="/staging/assets/img/2023-01-15-Underfitting-and-Regularization-Finding-the-Right-Balance/NetAug_SWE.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="generating-largest-augmented-model-via-single-path-one-shot-nas">Generating largest augmented model via Single Path One-Shot NAS</h2> <p>We propose this method to solve the problem of <a href="#generating_aug_model">generating the largest augmented model</a>. In NetAug, the largest augment model is generated randomly just based on the hyperparameters \(r\) and \(s\). Single Path One-Shot NAS with uniform sampling <d-cite key="guo2020single"></d-cite> revists the pitfalls of weight coupling in previous weight sharing methods. The one-shot paradigm is made attractive for real world tasks and better generalization. It is hyperparameter free, single path strategy works well because it can decouple the weights of different operations. This implementation can be more efficient in multiple search space. Using this technique we generate largest optimized augmented model by</p> <ol> <li> <p><strong>Supernet weight Optimization</strong> : \(\begin{equation}W_{\mathcal{A}}=\mathop{\arg \min}_{W} \: \mathcal{L}_{\text{train}}\big(\mathcal{N}(\mathcal{A},W)\big)\end{equation}\)</p> <p>The \(\mathcal{A}\) is the architecture search space represented as a directed acyclic graph which is encoded as a supernet \(\mathcal{N}(\mathcal{A}, W)\). During an SGD step in the above equation, each edge in the supernet graph is randomly dropped, using a dropout rate parameter. In this way, the co-adaptation of the node weights is reduced during training making the supernet training easier.</p> </li> <li> <p><strong>Architecture Search Optimization</strong> : \(\begin{equation}a^* = \mathop{\arg \max}_{a \in \mathcal{A}} \text{ ACC}_{\text{val}}\bigg(\mathcal{N}\big(a,W_{\mathcal{A}}(a)\big)\bigg)\end{equation}\)</p> <p>During search, each sampled architecture a inherits its weights from \(W_{\mathcal{A}}\) as \(W_{\mathcal{A}}(a)\). The architecture weights are ready to use making the search very efficient and flexible. This type of sequential optimization works because, the accuracy of any architecture \(a\) on a validation set using inherited weight \(W_{\mathcal{A}}(a)\) (without extra fine tuning) is highly predictive for the accuracy of \(a\) that is fully trained. Hence we try to minimize the training loss even further for better performance, supernet weights \(W_{\mathcal{A}}\) such that all the architectures in the search space are optimized simultaneously.</p> </li> </ol> \[\begin{equation}W_{\mathcal{A}} = \mathop{\arg \min}_{W} \: \mathbb{E}_{a \sim \Gamma(\mathcal{A})}\bigg[\mathcal{L}_{\text{train}}\big(\mathcal{N}(a, W(a))\big)\bigg]\end{equation}\] <p>where \(\Gamma(\mathcal{A})\) is the prior distribution of \(a \in \mathcal{A}\). This stochastic training of the supernet is helpful in better generalization of the optimized model and is also computationally efficient. To overcome the problem of weight coupling, the supernet \(\mathcal{N}(\mathcal{A},W)\) is chosen such that each architecture is a single path so that this realization is hyperparameter free as compared to traditional NAS approaches. The distribution \(\Gamma(\mathcal{A})\) is fixed apriori as a uniform distribution during our training and is not a learnable parameter.</p> <h2 id="evaluation-and-inference">Evaluation and Inference</h2> <p>Evaluation metric is the core for building any accurate machine learning model. We propose to implement the evaluation metrics precision@\(k\) ,recall@\(k\) and f1-score@\(k\) for the augmented sub-models sampled from largest augmented model and for the base model itself, where \(k\) represents the top k accuracy on the test set. These metrics will assist in evaluating the training procedure better than vanilla accuracy because we are trying to tackle the problem of underfitting, and an underfitted model is more likely to make the same prediction for every input and presence of class imbalance will lead to erroneous results.</p> <h2 id="conclusion">Conclusion</h2> <p>The main conclusion of this blog post is to further refine tiny neural networks effectively without any loss in accuracy and prevent underfitting in them. The paper implements this in a unique way apart from the conventional techniques such as regularization, dropout and data augmentation. This blog adds to other techniques that are orthogonal to NetAug can be used combined with NetAug for improvised results.</p> ]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In this blog post, we will go over the ICLR 2022 paper titled NETWORK AUGMENTATION FOR TINY DEEP LEARNING. This paper introduces a new training method for improving the performance of tiny neural networks. NetAug augments the network (reverse dropout), it puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision.]]></summary></entry><entry><title type="html">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/riit/" rel="alternate" type="text/html" title="Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning"/><published>2022-12-13T00:00:00+00:00</published><updated>2022-12-13T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/riit</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/riit/"><![CDATA[<blockquote> <p>QMIX [<a href="#8">8</a>], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be an weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art under the StarCraft Multi-Agent Challenge (SMAC) [<a href="#10">10</a>]. Further we found that the monotonicity constraint of QMIX is a key factor for its superior performance. We have open-sourced the code at https://github.com/xxxx/xxxx (Anonymous) for researchers to evaluate the effects of these proposed techniques. Our work has been widely used as a new QMIX baseline.</p> </blockquote> <h2 id="from-rl-to-marl"><a name="From_RL_to_MARL">From RL to MARL</a></h2> <p>Ever since AlphaZero beats humans at Go, RL has become a consistent hot spot in both academia and industry. The agent of RL can obtain some rewards by interacting with the environment and taking actions to maximize these cumulative rewards. Actually, almost all the RL problems can be described as <strong>Markov Decision Processes</strong> as illustrated in Figure <a href="#mdp">1</a>.</p> <div id="mdp" class="img-height-200 img-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/mdp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/mdp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/mdp-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/mdp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 1: The agent-environment interaction in a Markov decision process. (Image source: Sec. 3.1 Sutton &amp; Barto (2017) <a ref="#14">[14]</a>)).</center> <p><br/></p> <p>Just as its name implies, MARL contains multiple agents trained by RL algorithms in the same environment. Many complex multi-agent systems such as robot swarms control, autonomous vehicle coordination, and sensor networks, can be modeled as MARL tasks. The interaction of these agents would make them work together to achieve a common goal.</p> <div style="display:flex; margin:20px 0; gap:5px"><div id="chase" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/chase.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/chase.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/chase.gif-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/chase.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="magent" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/magent.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/magent.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/magent.gif-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/magent.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="hide" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/hide.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/hide.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/hide.gif-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/hide.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="smac" class="img-height-200"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/smac.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/smac.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/smac.gif-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/smac.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div></div> <div style="margin-bottom: 20px"><center>Figure 2: Some multi-agent cooperative scenarios [left-to-right]. <a href="https://github.com/openai/multiagent-particle-envs"> (a) Chasing in Multi-Agent Particle Environment (Predator-Prey); </a><br/> <a href="https://github.com/geek-ai/MAgent"> (b) MAgent Environment; </a> <a href="https://openai.com/blog/emergent-tool-use"> (c) Hide &amp; Seek; </a> <a href="https://github.com/oxwhirl/smac"> (d) StarCraft Multi-Agent Challenge. </a></center></div> <p>Actually, agents usually have a limited sight range to observe their surrounding environment. As the example shown in Figure <a href="#smac_obs">3</a>, the cyan border indicates the sight and shooting range of the agent, which means the agent could only obtain the information of terrain or other agents in that range. These kinds of multi-agent tasks can be modeled as decentralized partially observable Markov decision process (Dec-POMDP) [<a href="#6">6</a>], and the ultimate goal is to find a joint policy of agents \(\boldsymbol{\pi} = \langle \pi_{1},...,\pi_{n}\rangle\) to get the maximal global reward.</p> <div style="float:left; margin-right :40px"><div name="smac_obs" class="img-height-300"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/smac_agent_obs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/smac_agent_obs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/smac_agent_obs-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/smac_agent_obs.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 3: The partial observation of agents<br/>(Image source: SMAC <a ref="#10">[10]</a>). </center><br/></div> <p>Apparently, the main challenges stand between MARL and practical applications include the inherent communication constraints, partial observability, and the <em>Non-Stationarity</em> resulting from the changing policies of other agents. These challenges make it troublesome for agents to achieve better cooperation and lead to unstable learning. A setting known as <em>Centralized Training with Decentralized Execution</em> (CTDE) [<a href="#15">15</a>] has been proposed to meet these challenges. It trains the policies in a centralized way, which would access the global state \(S\) and local action-observation histories of all agents. However, each agent can only make its own decision based on its local action-observation history \(\tau^{i}\) during execution. The nonstationarity in training would be alleviated by learning a shared centralized value function for all agents. In the algorithms that integrate each agent’s \(Q_{i}\) together, QMIX is the representative and effective method to train the agents.<br/><br/></p> <h2 id="qmix-and-monotonicity-constraint"><a name="QMIX_and_Monotonicity_Constraint">QMIX and Monotonicity Constraint</a></h2> <p>To deal with the relationship between the individual agent and the cooperative group, QMIX [<a href="#8">8</a>] learns a joint action-value function \(Q_{tot}\), and factorizes the joint policy to the individual policy of each agent. In other words, as illustrated in Figure <a href="#frame">4</a>, QMIX integrates all the individual \(Q_{i}\) with a mixing network to obtain a centralized value function \(Q_{tot}\), which can be more appropriately updated by the global reward.</p> <div id="frame" class="img-height-310 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/qmix_frame-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/qmix_frame-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/qmix_frame-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/qmix_frame.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 4: Framework of QMIX. (Image source: QMIX <a ref="#8">[8]</a>) </center> <p><br/></p> <p>Still, it also can be represented in Eq.(\ref{eq1})</p> \[Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi) = g_{\phi}\left(s, Q_{1}\left(\tau^{1}, u^{1} ; \theta^{1}\right), \ldots, Q_{N}\left(\tau^{N}, u^{N} ; \theta^{N}\right)\right)\] \[\frac{\partial Q_{tot}(s, \boldsymbol{u} ; \boldsymbol{\theta}, \phi)}{\partial Q_{i}\left(\tau^{i}, u^{i}; \theta^{i}\right)} \geq 0, \quad \forall i \in \mathcal{N} \tag{1} \label{eq1}\] <p>where \(\theta^i\) is the parameter of the agent network \(i\), and \(\phi\) is the trainable parameter of the mixing network, which is responsible to factorize \(Q_{tot}\) to each agent \(Q_{i}\). The <em>Monotonicity Constraint</em> is implemented in the mixing network, which inputs the global state \(S\) and outputs nonnegative wights through <em>hypernetwork</em>. This delicate design ensures consistency between joint actions and the individual actions of each agent, then guarantees the Individual-Global-Max (IGM) principle. Benefiting from the monotonicity constraint in Eq. (\ref{eq1}), maximizing joint \(Q_{tot}\) is precisely the equivalent of maximizing individual \(Q_i\), which would also allow the optimal individual action to maintain consistency with optimal joint action. Furthermore, QMIX learns centralized value function \(Q_{tot}\) by sampling a multitude of transitions from the replay buffer and minimizing the mean squared temporal-difference (TD) error loss:</p> \[\mathcal{L}(\theta)= \frac{1}{2} \sum_{i=1}^{b}\left[\left(y_{i}^{}-Q_{tot}(s, u ; \theta, \phi)\right)^{2}\right] \tag{2} \label{eq2}\] <p>where the TD target value \(y=r+\gamma \underset{u^{\prime}}{\operatorname{max}} Q_{tot}(s^{\prime},u^{\prime};\theta^{-},\phi^{-})\), and \(\theta^{-}, \phi^{-}\) are the target network parameters copied periodically from the current network and kept constant for a number of iterations. Due to the strong constraints in Eq.(\ref{eq1}), QMIX is still criticized for the insufficient expressive capacity of the centralized critic [<a href="#3">3</a>].</p> <h2 id="extension-to-qmix"><a name="Extension_to_QMIX">Extension to QMIX</a></h2> <h3 id="experimental-design"><a name="Experimental_Design">Experimental Design</a></h3> <p>To facilitate the study of proper techniques affecting the training effectiveness and sample efficiency of QMIX, we perform a set of experiments designed to provide insight into some methods that have been proved effective in single-agent RL but may be ambiguous in MARL. In particular, we investigate the effects of: <strong>Adam optimizer with parallel rollout process; the incremental of replay buffer size; the number of parallel rollout process; \(\epsilon\)-exploration steps; the implementation of \(Q(\lambda)\) in centralized value function; the hidden size of agents’ recurrent network. And we also study the role of monotonicity constraints in QMIX.</strong> For all experiments, we generally use PyMARL [<a href="#10">10</a>] framework to implement QMIX. To ensure fairness we run independent five experimental trials for each evaluation, each with a random seed. Unless otherwise mentioned, we use default settings as in PyMARL whenever possible, while incorporating the techniques of interest. All results are plotted with the median and shaded the interval.</p> <p><strong>StarCraft Multi-Agent Challenge (SMAC)</strong> As a commonly used testing environment, SMAC [<a href="#10">10</a>] sets an example to offer a great opportunity to tackle the cooperative control problems in the multi-agent domain. We focus on the micromanagement challenge in SMAC, which means each agent is controlled by an independent agency that conditions on a limited observation area, and these groups of units are trained to conquer the enemy consisting of built-in AI. According to the quantity and type of enemy, all testing scenarios could be divided into <em>Easy, Hard</em>, and <em>Super-Hard</em> levels. Since QMIX can effectively solve the <em>Easy</em> tasks, we pay our attention to some <em>Hard</em> and <em>Super-Hard</em> scenarios that QMIX failed to win, especially in <em>Corridor, 3s5z_vs_3s6z</em>, and <em>6h_vs_8z</em>.</p> <p><strong>Predator-Prey (PP)</strong> is representative of another classical problem called <em>relative overgeneralization</em> [<a href="#16">16</a>] . The cooperating predators are trained to chase a faster running prey, and hope to capture this escaping robot with the fewest steps possible. We leverage Predator-Prey-2 (a variant of Predator-Prey), whose policy of prey is replaced with a hard-coded heuristic policy, asks the prey to move to the farthest sampled position to the predator. These two environments require greater cooperation between agents.</p> <h3 id="optimizer"><a name="Optimizer">Optimizer</a></h3> <p>As an important part of training neural networks, the selection of an optimizer is very important since it could seriously affect the training effect of the reinforcement learning agent. Without a further illustration, QMIX uses RMSProp [<a href="#21">21</a>] to optimize the neural networks of agents as they prove stable in SMAC. While Adam [<a href="#1">1</a>] is famous for the fast convergence benefiting from the momentum in training, which seems to be the first choice for AI researchers. We reckon that momentum property in Adam would have some advantages in learning the sampled data which is generated by agents interacting with the environment as in MARL. And then, on the other hand, QMIX is criticized for performing sub-optimally and sample inefficiency when equipped with the A2C framework, which is implemented to promote the training efficiency of the RL algorithm. VMIX [<a href="#12">12</a>] argues this limitation is brought about by the value-based inherent Q function, so they extend QMIX to the actor-critic style algorithm to take advantage of the A2C framework. This controversy attracts our attention to evaluate the performance of QMIX using Adam, as well as the parallel sampling paradigm.</p> <div id="optimizer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/optimizer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/optimizer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/optimizer-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/optimizer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 5: The Q networks optimized by Adam and RMSProp.</center> <p><br/></p> <p><strong>Results</strong> As shown in Figure <a href="#optimizer">5</a>, we run the Adam-supported QMIX with <strong>8 rollout processes</strong>. Different from what was described in VMIX, the performance and efficiency of QMIX could be greatly improved by Adam. We speculate the reason is the momentum property in Adam could fastly fit the newly sampled data from the parallel rollout processes and then enhance the performance, while RMSProp failed.</p> <h3 id="rollout-process-number"><a name="Rollout_Process_Number">Rollout Process Number</a></h3> <p>Naturally, we come to focus on the benefits of parallel data sampling in QMIX. A2C [<a href="#5">5</a>] provides an excellent example to reduce training time and improve the training efficiency in single-agent RL. As we implement the algorithms under the paradigm of A2C, there is usually a defined total number of samples and an unspecified number of rollout processes. The total number of samples \(S\) can be calculated as \(S = E \cdot P \cdot I\), where \(S\) is the total sum of sampled data, \(E\) is the number of samples in each episode, \(P\) and \(i\) is the number of rollout processes in parallel and policy iterations, respectively. This section aims to perform analysis and spur discussion on the impact of the parallel rollout process on the final performance of QMIX.</p> <div id="process_number" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/process_number-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/process_number-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/process_number-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/process_number.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 6: Given the total number of samples, fewer processes achieve better performance.</center> <p><br/></p> <p><strong>Results</strong> Still, we use Adam-supported QMIX to evaluate the effect of the number of the rollout process. Since we could choose the <em>Parallel</em> model to sample the interacting data of the agent with the environment in PyMARL, we can theoretically get more <strong>on-policy</strong> data which is close to the updating policy in training. Figure <a href="#process_number">6</a> shows that when \(S\) and \(P\) is given, the performance enhancement of QMIX is not consistent with the increase of rollout process number. The intuitive explanation is when we set the fewer number of rollout processes, the greater the quantity of policy would iterate [<a href="#14">14</a>]. Besides, too fast updated data in parallel may cause the factitious unstable training in policy updating, i.e., it is difficult for agents to learn effective information from rapidly sampled data from replay buffer. The more times policies are iterated, the more information the agents would learn and lead to an increase in performance. However, it also causes longer training time and loss of stability. We suggest trying the fewer rollout process in the beginning and then balancing between training time and performance.</p> <h3 id="replay-buffer-size"><a name="Replay_Buffer_Size">Replay Buffer Size</a></h3> <p>Replay buffer plays an important role in improving sample efficiency in off-policy single-agent RL. Its capacity would greatly affect the performance and stability of algorithms. Researchers usually set a very large capacity of replay buffer in Deep Q-network (DQN) [<a href="#4">4</a>] to stabilize the training. Some research of the effect of replay buffer in single-agent RL has already been carried out in [<a href="#22">22</a>] , which poses the distribution of sampled training data should be close as possible to the agents’ policies to be updated. Actually, there are two factors affected when we change the capacity of the replay buffer: (1) the replay capacity (total number of transitions/episodes stored in the buffer); and (2) the replay ratio (the number of gradient updates per environment transition/episode) of old policies. When we increase the capacity of replay buffer, the aged experiences of old policies would grow as the replay ratio fixed. Then the distribution of outdated experiences would also be much different from the updating policy, which would bring an additional difficulty to the training agents. From the results in [<a href="#22">22</a>], there seems to be an optimal range of choices between replay buffer size and replay ratio of experiences in RL, where we would like to know whether it is consistent with the results in MARL.</p> <div id="replay_buffer" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/buffer_size-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/buffer_size-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/buffer_size-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/buffer_size.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 7: Setting the replay buffer size to 5000 episodes allows for QMIX’s learning to be stable.</center> <p><br/></p> <p><strong>Results</strong> The results seem not to be consistent with that in single-agent RL. Figure <a href="#replay_buffer">7</a> shows the large replay buffer size of QMIX would cause instability during training. When we increase the buffer size from the default setting in PyMARL, the performance would almost continuously decline. We speculate the reason is the fast-changing distribution of experiences in a larger buffer would make it more difficult to fit sampled data due to the enormous joint action space. Since the samples become obsolete more quickly, these aged policies would also be more different from the updating policy, which brings additional difficulty. On the other hand, we find the same performance decline when we squeeze the buffer. We reckon that an insufficient buffer would accelerate the updating speed of sampling data in a disguised way, which makes it tough to fit the data and learn a good policy. We believe that researchers should be cautious to increase the buffer size in other multi-agent applications.</p> <h3 id="eligibility-traces"><a name="Eligibility_Traces">Eligibility Traces</a></h3> <p>The well-known trade-off between bias and variance of bootstrapping paradigm is a classic research topic in RL. Since we implement the Centralized Value Function (CVF) to alleviate the <em>Non-Stationarity</em> multi-agent settings, the estimated accuracy of CVF is critical to MARL and then guides the policies of agents to update. Eligibility traces such as TD(\(\lambda\))[<a href="#14">14</a>], Peng’s Q(\(\lambda\))[<a href="#2">2</a>], and TB(\(\lambda\))[<a href="#7">7</a>] achieve a balance between return-based algorithms (where return refers to the sum of discounted rewards \(\sum_{t} \gamma^{t} r_{t}\)) and bootstrap algorithms (where return refers \(r_t + V(s_{t+1})\)), then speed up the convergence of agents’ policies. As a pioneer, SMIX [<a href="#20">20</a>] equipped QMIX with the SARSA(\(\lambda\)) to estimate the accurate CVF and get decent performance. As another example of eligibility trace in Q-learning, we study the estimation of CVF using Peng’s Q\((\lambda)\) for QMIX.</p> <div id="qlambda1" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/td_lambda-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/td_lambda-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/td_lambda-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/td_lambda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 8: Q(λ) significantly improves performance of QMIX, but large values of λ lead to instability in the algorithm.</center> <p><br/></p> <p><strong>Results</strong> As the same in single-agent RL, the Q-networks without sufficient training usually have a large bias in bootstrapping returns. Figure <a href="#qlambda1">8</a> shows that, with the help of Q\((\lambda)\), the performance of QMIX has generally improved across all scenarios. It means the more accurate estimate of CVF would still provide a better direction of policy updating for each agent. However, the value of \(\lambda\) in Peng’s Q\((\lambda)\) is not so radical as in single-agent RL, which would lead to failed convergence due to the large variance. We recommend a small \(\lambda\), such as \(0.5\), when using \(Q(\lambda)\) in MARL.</p> <h3 id="hidden-size"><a name="Hidden_Size">Hidden Size</a></h3> <p>Searching for an optimal scale and architecture of neural networks is a very tough problem in the field of machine learning. Researchers typically use empirically small networks to train the agents in deep reinforcement learning. Since the role of neural networks is to extract the features of input states and actions, the size of the neural network would also have a great impact on the performance of MARL algorithms. The study in [<a href="#23">23</a>] has revealed that networks with a complex structure like ResNet[<a href="#25">25</a>] and DenseNet[<a href="#26">26</a>] can extract more useful information for training, while Ba [<a href="#24">24</a>] poses the width of neural networks is probably more important than its depth. The subsequent study on QMIX [<a href="#19">19</a>] makes preliminary research on the depth of neural networks, which showed a limited improvement in performance. Though, there is little research on the width of neural networks in MARL. Instead of searching for an optimal network architecture here, we just want to make a pilot study on the effect of the hidden size of network width in QMIX.</p> <div id="hiddensize" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/hidden_size-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/hidden_size-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/hidden_size-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/hidden_size.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 9: Impact of hidder size of network in QMIX.</center> <p><br/></p> <p><strong>Results</strong> The study in [<a href="#24">24</a>] illustrates the ability of infinity width networks to fit any complex function, which would theoretically provide the performance gain from increasing network width. As shown in Figure <a href="#hiddensize">9</a>, the final performance or the efficiency of policy training would have varying degrees of improvement when we increase the hidden size of the network from 64 to 256 in QMIX, where <strong>QMIX-ALL-Hidden</strong> refers to the size of the network including RNN and mixing part, while <strong>QMIX-RNN-Hidden</strong> just refers to RNN. Also, the results reveal the spectacular effect of increasing the network width of RNN, which would allow for about a 20% increase in the Super-Hard scenarios <em>3s5z_vs_3s6z</em>. While the performance improvement is limited in enlarging the mixing network. We speculate that more units in the network are needed to represent the complex temporally context information in RNN, which is not included in the mixing network. We advise researchers to appropriately increase the network width of RNN to achieve better performance.</p> <h3 id="exploration-steps"><a name="Exploration_Steps">Exploration Steps</a></h3> <p>Exploration and exploitation are other classic trade-offs in reinforcement learning. Agents need some directed mechanisms to explore the states that may be of higher value or inexperienced. The most versatile method of exploration in RL is \(\epsilon\)-greedy action, which makes the agent select random actions with probability \(\epsilon\), or select the greedy action with \(1 - \epsilon\). The value of \(\epsilon\) would drop down with training and then stays at a small constant. This exploration mechanism is usually implemented for each agent to select their action, which has been criticized by MAVEN [<a href="#3">3</a>] about lacking joint exploratory policy over an entire episode. However, we can still get more exploration when \(\epsilon\) drops slower, then we evaluate the performance of the annealing period of \(\epsilon\)-greedy in some Super-Hard scenarios in SMAC.</p> <div id="exploration" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/exploration-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/exploration-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/exploration-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/exploration.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 10: Experinments for ε anneal period.</center> <p><br/></p> <p><strong>Results</strong> Apparently, appropriately increasing the annealing period of \(\epsilon\)-greedy from 100K steps to 500K would get explicit performance gain in those hard exploration scenarios, where QMIX failed with the default setting. However, as shown in Figure <a href="#exploration">10</a>, too large steps like 1000K would also bring additional exploration noise even make the training collapse. The results above confirm the \(\epsilon\)-greedy mechanism is still the proper and simplest choice in MARL but should be elaboratively tuned for different tasks.</p> <h3 id="integrating-the-techniques"><a name="Integrating_the_Techniques">Integrating the Techniques</a></h3> <p>These techniques mentioned above indeed impacts QMIX in hard cooperative scenarios of SMAC, which really catches our attention to exhaust the extreme performance of QMIX. We combine these techniques and finetune all the hyperparameters in QMIX for each scenario of SMAC. As shown in Table <a href="#table1">1</a>, the Finetuned-QMIX would almost conquer all the scenarios in SMAC and exceed the effect of the original QMIX with a large margin in some Hard and Super-Hard scenarios.</p> <p><a name="table1"> </a></p> <center> Table 1: Best median test win rate of Finetuned-QMIX and QMIX (batch size=128) in all scenarios. </center> <table style="text-align: center; width: 600px; margin: 0 auto; margin-bottom:20px; margin-top:20px"> <thead> <tr> <th>Senarios</th> <th>Difficulty</th> <th>QMIX</th> <th>Finetuned-QMIX</th> </tr> </thead> <tbody> <tr> <td>10m_vs_11m</td> <td>Easy</td> <td>98%</td> <th>100%</th> </tr> <tr> <td>8m_vs_9m</td> <td>Hard</td> <td>84%</td> <th>100%</th> </tr> <tr> <td>5m_vs_6m</td> <td>Hard</td> <td>84%</td> <th>90%</th> </tr> <tr> <td>3s_vs_5z</td> <td>Hard</td> <td>96%</td> <th>100%</th> </tr> <tr> <td>bane_vs_bane</td> <td>Hard</td> <th>100%</th> <th>100%</th> </tr> <tr> <td>2c_vs_64zg</td> <td>Hard</td> <th>100%</th> <th>100%</th> </tr> <tr> <td>corridor</td> <td>Super hard</td> <td>0%</td> <th>100%</th> </tr> <tr> <td>MMM2</td> <td>Super hard</td> <td>98%</td> <th>100%</th> </tr> <tr> <td>3s5z_vs_3s6z</td> <td>Super hard</td> <td>3%</td> <th>93% (Hidden Size = 256)</th> </tr> <tr> <td>27m_vs_3s6z</td> <td>Super hard</td> <td>56%</td> <th>100%</th> </tr> <tr> <td>6h_vs_8z</td> <td>Super hard</td> <td>0%</td> <th>93% (λ = 0.3)</th> </tr> </tbody> </table> <h2 id="role-of-monotonicity-constraint"><a name="Role_of_Monotonicity_Constraint">Role of Monotonicity Constraint</a></h2> <h3 id="amazing-performance-in-policy-based-methods"><a name="Amazing_Performance_in_Policy-Based_Methods">Amazing Performance in Policy-Based Methods</a></h3> <div id="qmix_sy" class="img-height-180 image-center img-margin-left-30"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/riit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/riit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/riit-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/riit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 11: Architecture for AC-MIX: |·| denotes absolute value operation, implementing the monotonicity constraint of QMIX. <b>W</b> denotes the non-negative mixing weights. Agent i denotes the agents network, which can be trained end-to-end by maximizing the $$Q_{tot}$$. </center> <p>The novelty of QMIX is the IGM continuity between \(\text{argmax} Q_{tot}\) and \(\text{argmax} \sum_{i}^{N} Q_{i}\), which is implemented in the mixing network. <strong>We still expect to study the role of <em>monotonicity constraint</em> in MARL</strong>. Therefore, we propose an actor-critic style algorithm called Actor-Critic-Mixer (AC-MIX), which has a similar architecture to QMIX. As illustrated in Figure <a href="#qmix_sy">11</a>, we use the monotonic mixing network as a centralized critic, which integrates \(Q_{i}\) of each agent, to optimize the decentralized policy networks \(π^i_{θ_i}\) in an end-to-end pattern. We still add the Adaptive Entropy <a href="#18">[18]</a> of each agent in the optimization object of Eq. \ref{eq3} to get more exploration, and the detail of the algorithm will be described in Appendix <a href="#A">A</a>.</p> \[\max _{\theta} \mathbb{E}_{t, s_{t}, \tau_{t}^{1}, \ldots, \tau_{t}^{n}}\left[Q_{\theta_{c}}^{\pi}\left(s_{t}, \pi_{\theta_{1}}^{1}\left(\cdot \mid \tau_{t}^{1}\right), \ldots, \pi_{\theta_{n}}^{n}\left(\cdot \mid \tau_{t}^{n}\right)\right) + \mathbb{E}_{i}\left[\mathcal{H}\left(\pi_{\theta_{i}}^{i}\left(\cdot \mid \tau_{t}^{i}\right)\right)\right]\right] \tag{3} \label{eq3}\] <div id="riit_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_riit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_riit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_riit-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/monotonicity_riit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 12: Comparing AC-MIX w./ and w./o. monotonicity constraint (remove absolute value operation) on SMAC and Predator-Prey-2</center> <p><br/></p> <p>As the monotonicity constraint on the critic of AC-MIX is theoretically no longer required as the critic is not used for greedy action selection. We can evaluate the effects of the monotonicity constraint by removing the absolute value operation in the mixing network. The results in Figure <a href="#riit_abla">12</a> demonstrate the <em>monotonicity constraint</em> significantly improves the performance of AC-MIX. Then to explore the generality of <em>monotonicity constraints</em> in the parallel sampling framework of MARL, we extend the above experiments to VMIX [<a href="#12">12</a>] . VMIX adds the monotonicity constraint to the value network of A2C, and learns the policy of each agent by advantage-based policy gradient [<a href="#14">14</a>] as illustrated in Figure <a href="#vmix_net">13</a>. Still, the result from Figure <a href="#vmix_abla">14</a> shows that the monotonicity constraint improves the sample efficiency in value networks.</p> <div id="vmix_net" class="img-height-180 image-center img-margin-left-60"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/vmix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/vmix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/vmix-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/vmix.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 13. Architecture for VMIX: |·| denotes absolute value operation</center> <p><br/></p> <div id="vmix_abla" class="img-height-210 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_vmix-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_vmix-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/monotonicity_vmix-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/monotonicity_vmix.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 14: Comparing VMIX w./ and w./o. monotonicity constraint (remove absolute value operation) on SMAC</center> <p><br/></p> <h3 id="what-is-under-the-hood"><a name="What_is_Under_the_Hood">What is Under the Hood?</a></h3> <p>Observed from the results of previous experiments, <strong>the <em>monotonicity constraints</em> in the mixing network indeed improve performance and sample efficiency of training</strong>, but on the flip side of the coin, QMIX is still criticized for the insufficient expressive capacity of the centralized critic [<a href="#3">3</a>], which may cause poor performance. The abnormal question naturally occurred to us: <em>Why the performance of AC-MIX would be better than AC-MIX-nonmonotonic aims to relax the monotonicity constraint of mixing network</em>?</p> <p>To answer these two questions we first need to reexamine the IGM principle. Since the monotonicity in QMIX is defined as a constraint on the relationship between \(Q_{tot}\) and each \(Q_{i}\) :</p> \[Q_{tot} = \sum_{i=1}^{N}w_{i}(s_{t}) \cdot Q_{i} + b(s_{t}), \\ w_{i} = \frac{\partial Q_{tot}}{\partial Q_{i}} \geq 0, \forall i \in A. \tag{4} \label{eq4}\] <p>From the sufficient condition above, the weight \(w_{i}\) generated by <em>hyper-network</em> would be forced to be greater or equal to zero. To put it another way, it makes the parameter space smaller for searching \(w_{i}\) weights. As illustrated in the schematic diagram <a href="#diagram">15</a> with just two agents, assume the red region is the original search space, the restricted search space of \(w_{i}\) is the blue region in the first quadrant. Then the optimal solution in the original domain cannot be expressed correctly in the restricted region. On the other hand, the search area of exhausting the whole joint state-action space would also be decreased exponentially by \((\frac{1}{2})^{N}\) (\(N\) demotes the number of \(w_{i}\), as well as the number of agents). Since the essence of learning in MARL is to search for the optimal joint-policy parameterized by weights and bias of agents and mixing network, QMIX could find a satisfying policy more quickly in the reduced parameter space.</p> <div id="diagram" class="img-height-400 image-center img-margin-left-150"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/diagram-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/diagram-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/diagram-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/diagram.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <center>Figure 15: Diagram of parameter searching space of two agents in QMIX</center> <p><br/></p> <p>As a side effect, the global optimum may not be in the parameter space that QMIX needs to search at all due to the monotonicity of the mixing network. One effective way is to estimate the \(Q_{tot}\) as accurately as possible in the hope that it could find the global optimum, this probably explains why \(Q(\lambda)\) in the previous section could result in such a performance improvement in SMAC. On the other hand, we could delicately design the reward function to be approximate monotonic when we use QMIX to solve cooperative multi-agent tasks. Then adapting the algorithm to the test environment is not a good idea, after all, we still need to figure out how to use QMIX more effectively or develop other more efficient algorithms.</p> <h2 id="appendix"><a name="Appendix">Appendix</a></h2> <h3 id="a-pseudo-code-of-ac-mix-">A Pseudo-code of AC-MIX<a id="A"> </a></h3> <p>In this section, we show the pseudo-code for the training procedure of AC-MIX. (1) Training the critic network with offline samples and 1-step TD error loss improves the sample efficiency for critic networks; (2) We find that policy networks are sensitive to old samples reuse. Training policy networks end-to-end and critic with TD(\(\lambda\)) and online samples improve the learning stability of AC-MIX.</p> <div id="algorithm_riit" class="img-height-600 image-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-13-riit/algorithm_riit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-13-riit/algorithm_riit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-13-riit/algorithm_riit-1400.webp"/> <img src="/staging/assets/img/2022-12-13-riit/algorithm_riit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="b-hyperparameters">B HYPERPARAMETERS</h3> <p>In this section, we present our hyperparameters tuning process. We get the optimal hyperparameters for each algorithm by grid search, shown in Table <a href="#t5">5</a>.</p> <center> Table 5: Hyperparameters Search on SMAC. The bold type indicates the selected hyperparameters. </center> <table style="text-align: center; width: 700px; margin: 0 auto; margin-bottom:20px; margin-top:20px;"><a name="t5"> </a> <thead> <tr> <th>Tricks</th> <th>QMIX</th> <th>AC-MIX</th> </tr> </thead> <tbody> <tr> <td>Optimizer</td> <td><b>Adam</b>,RMSProp</td> <td><b>Adam</b>,RMSProp</td> </tr> <tr> <td>Learning Rates</td> <td>0.0005, <b>0.001</b></td> <td>0.0005, <b>0.001</b></td> </tr> <tr> <td>Batch Size (episodes)</td> <td>32, 64, <b>128</b></td> <td>32, <b>64</b> </td> </tr> <tr> <td>Replay Buffer Size</td> <td><b>5000</b>, 10000, 20000</td> <td>2000, <b>5000</b>, 10000</td> </tr> <tr> <td>Q(λ)/TD(λ)</td> <td>0, 0.3, <b>0.6</b>, 0.9</td> <td>0.3, <b>0.6</b>, 0.8</td> </tr> <tr> <td>Entropy/Adaptive Entropy</td> <td>-</td> <td>0.005, 0.01, <b>0.03</b>, 0.06</td> </tr> <tr> <td>ε Anneal Steps</td> <td>50K, <b>100K, 500K</b>, 1000K</td> <td>-</td> </tr> </tbody> </table> <p><br/></p> <p><strong>Rollout Processes Number</strong>. For SMAC, 8 rollout processes for parallel sampling are used to obtain as many samples as possible from the environments at a high rate. And, 4 rollout processes are used for Predator-Prey-2.</p> <p><strong>Other Settings</strong>. We set all discount factors \(\gamma\) = 0.99. We update the target network every 200 episodes.</p> <h2 id="reference"><a name="Reference">Reference</a></h2> <p><a name="1" href="https://arxiv.org/abs/1412.6980">[1] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 2015. </a></p> <p><a name="2" href="https://arxiv.org/abs/2103.00107">[2] Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney, Michal Valko, and David Abel. Revisiting peng’s q (λ) for modern reinforcement learning. arXiv preprint arXiv:2103.00107, 2021. </a></p> <p><a name="3" href="https://arxiv.org/abs/1910.07483">[3] Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-agent variational exploration. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,pp. 7611–7622, 2019. </a></p> <p><a name="4" href="https://arxiv.org/abs/1312.5602">[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</a></p> <p><a name="5" href="http://proceedings.mlr.press/v48/mniha16.html">[5] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928–1937, 2016.</a></p> <p><a name="6" href="https://www.comp.nus.edu.sg/~leews/publications/rss09.pdf">[6] Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Pomdps for robotic tasks with mixed observability. 5:4, 2009. </a></p> <p><a name="7" href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&amp;context=cs_faculty_pubs">[7] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In ICML 2000, Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp.759–766. Morgan Kaufmann, 2000. </a></p> <p><a name="8" href="http://proceedings.mlr.press/v80/rashid18a.html">[8] Tabish Rashid, Mikayel Samvelyan, Christian Schr ̈oder de Witt, Gregory Farquhar, Jakob N.Foerster, and Shimon Whiteson. QMIX: monotonic value function factorization for deep multi-agent reinforcement learning. In ICML 2018, Stockholmsmassan, Stockholm, Sweden, July10-15, 2018, pp. 4292–4301, 2018. </a></p> <p><a name="9" href="https://ui.adsabs.harvard.edu/abs/2020arXiv200610800R/abstract">[9] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: Expand-ing Monotonic Value Function Factorisation. arXiv preprint arXiv:2006.10800, 2020. </a></p> <p><a name="10" href="https://arxiv.org/abs/1902.04043">[10] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, NantasNardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and ShimonWhiteson. The StarCraft Multi-Agent Challenge.arXiv preprint arXiv:1902.04043, 2019. </a></p> <p><a name="11" href="http://proceedings.mlr.press/v97/son19a.html">[11] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN: learning to factorize with transformation for cooperative multi-agent reinforcement learning. In ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5887–5896, 2019. </a></p> <p><a name="12" href="https://www.aaai.org/AAAI21Papers/AAAI-2412.SuJ.pdf">[12] Jianyu Su, Stephen Adams, and Peter A. Beling. Value-Decomposition Multi-Agent Actor-Critics. arXiv:2007.12306, 2020. </a></p> <p><a name="13" href="https://arxiv.org/abs/1706.05296">[13] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-pel. Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv preprint arXiv:1706.05296, 2017. </a></p> <p><a name="14" href="https://go.gale.com/ps/i.do?id=GALE%7CA61573878&amp;sid=googleScholar&amp;v=2.1&amp;it=r&amp;linkaccess=abs&amp;issn=07384602&amp;p=AONE&amp;sw=w">[14] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. </a></p> <p><a name="15" href="https://arxiv.org/abs/2008.01062">[15] Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex Dueling Multi-Agent Q-Learning. arXiv:2008.01062, 2020. </a></p> <p><a name="16" href="https://www.aaai.org/ocs/index.php/SSS/SSS18/paper/viewPaper/17508">[16] Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent Soft Q-Learning. arXivpreprint arXiv:1804.09817, 2018. </a></p> <p><a name="17" href="https://arxiv.org/abs/2002.03939">[17] Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and HongyaoTang. Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning. arXiv preprint arXiv:2002.03939, 2020. </a></p> <p><a name="18" href="https://arxiv.org/abs/2010.09776">[18] Ming Zhou, Jun Luo, and Julian Villella et al. Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving, 2020. </a></p> <p><a name="19" href="https://www.jmlr.org/papers/volume21/20-081/20-081.pdf">[19] Rashid T, Samvelyan M, Schroeder de Witt C, et al. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 2020, 21.</a></p> <p><a name="20" href="https://ojs.aaai.org/index.php/AAAI/article/view/6223">[20] Wen C, Yao X, Wang Y, et al. Smix (λ): Enhancing centralized value functions for cooperative multi-agent reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(05): 7301-7308. </a></p> <p><a name="21" href="http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf">[21] Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 2012, 14(8): 2.</a></p> <p><a name="22" href="http://proceedings.mlr.press/v119/fedus20a.html">[22] Fedus W, Ramachandran P, Agarwal R, et al. Revisiting fundamentals of experience replay. International Conference on Machine Learning. PMLR, 2020: 3061-3071. </a></p> <p><a name="23" href="http://proceedings.mlr.press/v119/ota20a.html">[23] Ota K, Oiki T, Jha D, et al. Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?. International Conference on Machine Learning. PMLR, 2020: 7424-7433.</a></p> <p><a name="24" href="https://arxiv.org/abs/1312.6184">[24] Ba L J, Caruana R. Do deep nets really need to be deep?. arXiv preprint arXiv:1312.6184, 2013.</a></p> <p><a name="25" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">[25] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p> <p><a name="26" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html">[26] Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks. Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.</a></p> <p><a name="27" href="http://proceedings.mlr.press/v48/wangf16.html">[27] Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning. International conference on machine learning. PMLR, 2016: 1995-2003.</a></p>]]></content><author><name></name></author><category term="multi-agent"/><category term="reinforcement-learning"/><category term="experimental techniques"/><category term="monotonicity"/><summary type="html"><![CDATA[QMIX [8], a very classical multi-agent reinforcement learning (MARL) algorithm, is often considered to be an weak performance baseline due to its representation capability limitations. However, we found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art under the StarCraft Multi-Agent Challenge (SMAC) [10]. Further we found that the monotonicity constraint of QMIX is a key factor for its superior performance. We have open-sourced the code at https://github.com/xxxx/xxxx (Anonymous) for researchers to evaluate the effects of these proposed techniques. Our work has been widely used as a new QMIX baseline.]]></summary></entry><entry><title type="html">Decay No More</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/adamw/" rel="alternate" type="text/html" title="Decay No More"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/adamw</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/adamw/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Weight decay is a regularization technique in machine learning which scales down the weights in every step. It dates back at least to the 1990’s and the work of Krogh and Hertz <d-cite key="Krogh1991"></d-cite> and Bos and Chug <d-cite key="Bos1996"></d-cite>.</p> <p>In <code class="language-plaintext highlighter-rouge">Pytorch</code>, weight decay is one simple line which typically is found somewhere in the <code class="language-plaintext highlighter-rouge">step</code>-method:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s">'params'</span><span class="p">]:</span>
  <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">decay</span><span class="p">)</span></code></pre></figure> <p>Subtracting a multiple of the weight can be seen as taking a step into the negative gradient direction of the squared norm of the weight. This relates weight decay to \(\ell_2\)-regularization (see also the <a href="#appendix">Appendix</a> with an excerpt of the original work by Krogh and Hertz <d-cite key="Krogh1991"></d-cite>).</p> <p>The exact mechanism of weight decay is still puzzling the machine learning community:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">The story of weight decay in pictures:<br/><br/>weight decay ...<br/>1) improves data efficiency by &gt; 50%<br/>2) is frequently found in the best hyperparam configs<br/>3) is among the most important hparams to tune<br/>4) is also tricky to tune <a href="https://t.co/PjWpk3pJxz">pic.twitter.com/PjWpk3pJxz</a></p>&mdash; Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1614327550058328064?ref_src=twsrc%5Etfw">January 14, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">There is a gaping hole in the literature regarding the purpose of weight decay in deep learning. Nobody knows what weight decay does! AFAIK, the last comprehensive look at weight decay was this 2019 paper <a href="https://t.co/7WDBZojsm0">https://t.co/7WDBZojsm0</a>, which argued that weight decay <a href="https://t.co/qUpCbfhFRf">https://t.co/qUpCbfhFRf</a></p>&mdash; Jeremy Cohen (@deepcohen) <a href="https://twitter.com/deepcohen/status/1617274166570528769?ref_src=twsrc%5Etfw">January 22, 2023</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>The paper by Zhang et al. <d-cite key="Zhang2019"></d-cite> - which is the one mentioned in the second tweet - gives a comprehensive overview of weight decay and its effect on generalization, in particular in the interplay with Batch Normalization <code class="language-plaintext highlighter-rouge">(BN)</code> <d-cite key="Ioffe2015"></d-cite>. Batch Normalization describes a module of a network that normalizes the output of the previous layer to have zero mean and variance of one (or a variant of this with learnable mean and variance). We will not go into the details here but refer to <a href="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/">this blog post</a> <d-cite key="pieterjan2022normalizationisdead"></d-cite> for the interested reader.</p> <p>We want to summarize two findings of <d-cite key="Zhang2019"></d-cite>:</p> <ul> <li>On the one hand, weight decay has (in theory) no effect on layers with <code class="language-plaintext highlighter-rouge">(BN)</code>. This is simply due to the fact that <code class="language-plaintext highlighter-rouge">(BN)</code> makes the output invariant to a rescaling of the weights.</li> </ul> <blockquote> Weight decay is widely used in networks with Batch Normalization (Ioffe &amp; Szegedy, 2015). In principle, weight decay regularization should have no effect in this case, since one can scale the weights by a small factor without changing the network’s predictions. Hence, it does not meaningfully constrain the network’s capacity. —Zhang et al., 2019 </blockquote> <ul> <li>However, te experiments of the paper show that weight decay on layers with <code class="language-plaintext highlighter-rouge">(BN)</code> can nevertheless improve accuracy. The authors argue that this is due to an effectively larger learning rate.</li> </ul> <p>This blog post will summarize the development of weight decay specifically for <span style="font-family:monospace">Adam</span>. We try to shed some light on the following questions:</p> <ol> <li>What is the difference between <span style="font-family:monospace">Adam</span> and its weight decay version <span style="font-family:monospace">AdamW</span>? Does the existing literature give a clear answer to the question when (and why) <span style="font-family:monospace">AdamW</span> performs better?</li> <li>Is the weight decay mechanism of <span style="font-family:monospace">AdamW</span> just <em>one more trick</em> or can we actually motivate it from an optimization perspective?</li> <li>The last section is somewhat explorational: could we come up with different formulas for a weight decay version of <span style="font-family:monospace">Adam</span>? By doing so, we will see that <span style="font-family:monospace">AdamW</span> already combines several advantages for practical use.</li> </ol> <h3 id="notation">Notation</h3> <p>We denote by \(\alpha &gt; 0\) the initial learning rate. We use \(\eta_t &gt; 0\) for a learning rate schedule multiplier. By this, the effective learning rate in iteration \(t\) is \(\alpha \eta_t\). We use \(\lambda &gt; 0\) for the weight decay parameter.</p> <h2 id="adam">Adam</h2> <p><span style="font-family:monospace">Adam</span> uses an exponentially moving average (EMA) of stochastic gradients, typically denoted by \(m_t\), and of the elementwise squared gradients, denoted by \(v_t\).</p> <p>We denote with \(\hat m_t\) and \(\hat v_t\) the EMA estimates with bias correction (see <d-cite key="Kingma2015"></d-cite>), this means</p> \[\hat m_t = \frac{m_t}{1-\beta_1^t}, \quad \hat v_t = \frac{v_t}{1-\beta_2^t}\] <p>where \(\beta_1, \beta_2 \in [0,1)\). The update formula of <span style="font-family:monospace">Adam</span> is given by</p> \[w_t = w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>How would <span style="font-family:monospace">Adam</span> handle regularization? The first approach to this was to simply add the regularization term \(\frac{\lambda}{2}\|w\|^2\) on top of the loss, do backpropagation and then compute the <span style="font-family:monospace">Adam</span> step as outlined above. This is usually referred to as <span style="font-family:monospace">AdamL2</span>. However, Loshchilov and Hutter <d-cite key="Loshchilov2019"></d-cite> showed that this can be suboptimal and one major contribution to alleviate this was the development of <span style="font-family:monospace">AdamW</span>.</p> <h2 id="adamw">AdamW</h2> <p>For training with \(\ell_2\)-regularization, Loshchilov and Hutter proposed <span style="font-family:monospace">AdamW</span> in 2019 <d-cite key="Loshchilov2019"></d-cite> as an alternative to <span style="font-family:monospace">AdamL2</span>. In the paper, the update formula is given as</p> \[\tag{AdamW} w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>While for <span style="font-family:monospace">Adam</span> several results for convex and nonconvex problems are established <d-cite key="Defossez2022, Reddi2018"></d-cite>, theoretical guarantees for <span style="font-family:monospace">AdamW</span> have been explored - to the best of our knowledge - only very recently <d-cite key="Anonymous2023"></d-cite>. Despite this, the method has enjoyed considerable practical success: for instance, <span style="font-family:monospace">AdamW</span> is implemented in the machine learning libraries Tensorflow <d-cite key="Abadi2015"></d-cite> and Pytorch <d-cite key="Paszke2019"></d-cite>. Another example is the <code class="language-plaintext highlighter-rouge">fairseq</code> library, developped by Facebook Research, which implements many SeqToSeq models. In their codebase, when <span style="font-family:monospace">Adam</span> is specified with weight decay, <span style="font-family:monospace">AdamW</span> is used by default (see <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/optim/adam.py">here</a>).</p> <p>We summarize the empirical findings of <d-cite key="Loshchilov2019"></d-cite> as follows:</p> <ul> <li> <p><span style="font-family:monospace">AdamW</span> improves generalization as compared to <span style="font-family:monospace">AdamL2</span> for image classification tasks. In the paper, the authors use a ResNet model <d-cite key="He2016"></d-cite> for the CIFAR10 and Imagenet32 dataset.</p> </li> <li> <p>Another advantage of <span style="font-family:monospace">AdamW</span> is stated in the abstract of <d-cite key="Loshchilov2019"></d-cite>:</p> </li> </ul> <blockquote> We provide empirical evidence that our proposed modification decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam [...]. —Loshchilov and Hutter, 2019 </blockquote> <p>What the authors mean by <em>decoupling</em> is that if we plot the test accuracy as a heatmap of learning rate and weight decay, the areas with high accuracy are more rectangular; the best learing rate is not too sensitive to the choice of weight decay. We illustrate this conceptually in the plot below which is inspired by Figure 2 in <d-cite key="Loshchilov2019"></d-cite>. The advantage of a decoupled method is that if one of the two hyperparameters is changed, the optimal value for the other one might still be identical and does not need to be retuned - this could reduce a 2D grid search to two 1D line searches.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-adamw/heatmap-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-adamw/heatmap-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-adamw/heatmap-1400.webp"/> <img src="/staging/assets/img/2022-12-01-adamw/heatmap.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig. 1: Heatmap of the test accuracy (bright = good accuracy) depending on learning rate and weight decay parameter choice. </div> <p>When revisiting the literature on <span style="font-family:monospace">AdamW</span> we made an interesting practical observation: the <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">Pytorch implementation</a> of <span style="font-family:monospace">AdamW</span> is actually slightly different to the algorithm proposed in the paper. In Pytorch, the following is implemented:</p> \[w_t = (1-\eta_t \alpha \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>The difference is that the decay factor in the code is \(1-\eta_t \alpha \lambda\) instead of \(1-\eta_t \lambda\) in the paper. Clearly, this is equivalent as we can simply reparametrize the weight decay factor \(\lambda\) to make up for this. However, as the default learning rate \(\alpha=0.001\) is rather small, this means that practicioners might need to choose rather high values of \(\lambda\) in order to get sufficiently strong decay. Moreover, this leaves a certain ambiguity when tuned values for \(\lambda\) are reported in the literature.</p> <h2 id="follow-up-work">Follow-up work</h2> <p>In a recent article, Zhuang et al. revisit the <span style="font-family:monospace">AdamW</span> method and try to explain its practical success <d-cite key="Zhuang2022"></d-cite>. One of their central arguments is that <span style="font-family:monospace">AdamW</span> is approximately equal to <span style="font-family:monospace">Adam</span> with a proximal update for \(\ell_2\)-regularization.</p> <p>Before explaining this in detail, we first want to summarize the empirical findings of <d-cite key="Zhuang2022"></d-cite>:</p> <ul> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>deactivated</em>, <span style="font-family:monospace">AdamW</span> achieves better generalization compared to <span style="font-family:monospace">AdamL2</span> for image classification with a standard ResNet architecture <d-cite key="He2016"></d-cite>.</li> <li>When <code class="language-plaintext highlighter-rouge">(BN)</code> is <em>activated</em>, the test accuracy of <span style="font-family:monospace">AdamW</span> and <span style="font-family:monospace">AdamL2</span> are on par. Moreover, the best accuracy is achieved for no weight decay, i.e. \(\lambda=0\).</li> </ul> <p>The second result is somewhat stunning as it seems to contradict the results in <d-cite key="Loshchilov2019"></d-cite>, which had shown that <span style="font-family:monospace">AdamW</span> generalizes better than <span style="font-family:monospace">AdamL2</span>.<d-footnote>It seems like the AdamW-paper also used (BN) in their experiments, see https://github.com/loshchil/AdamW-and-SGDW.</d-footnote></p> <p>Comparing the details of the experimental setups, we presume the following explanations for this:</p> <ul> <li> <p>The model that is trained in <d-cite key="Loshchilov2019"></d-cite> is slightly different as it uses a Shake-Shake-Image ResNet <d-cite key="He2016, Gastaldi2017"></d-cite>.</p> </li> <li> <p>From Figure 4 in <d-cite key="Loshchilov2019"></d-cite>, one can observe that the improvement in accuracy for the CIFAR-10 dataset becomes noticeable very late in the training (see also Section 4.3 in <d-cite key="Loshchilov2019"></d-cite>). Thus, depending on the number of epochs after which training is stopped, one can reach different conclusions.</p> </li> </ul> <h2 id="proxadam">ProxAdam</h2> <p>The paper by Zhuang et al. <d-cite key="Zhuang2022"></d-cite> does not only compare <span style="font-family:monospace">AdamL2</span> to <span style="font-family:monospace">AdamW</span> experimentally, but it also provides a mathematical motivation for weight decay. In order to understand this, we first need to introduce the <strong>proximal operator</strong>, a central concept of convex analysis.</p> <h3 id="a-short-introduction-to-proximal-operators">A short introduction to proximal operators</h3> <p>Proximal algorithms have been studied for decades in the context of (non-smooth) optimization, way before machine learning was a thing. The groundwork of this field has been laid by R. Tyrrell Rockafellar from the 1970’s onwards <d-cite key="Rockafellar1976,Rockafellar1998"></d-cite>. If \(\varphi: \mathbb{R}^n \to \mathbb{R}\) is convex then the proximal operator is defined as</p> \[\mathrm{prox}_\varphi(x) := \mathrm{argmin}_{z \in \mathbb{R}^n} \varphi(z) + \frac12 \|z-x\|^2.\] <p>For many classical regularization functions (e.g. the \(\ell_1\)-norm), the proximal operator can be computed in closed form. This makes it a key ingredient of optimization algorithms for regularized problems. Assume that we want to minimize the sum of a differentiable loss \(f\) and a convex regularizer \(\varphi\), i.e.</p> \[\min_{w \in \mathbb{R}^n} f(w) + \varphi(w).\] <p>The proximal gradient method in this setting has the update formula</p> \[w_{t} = \mathrm{prox}_{\alpha \varphi}\big(w_{t-1}- \alpha \nabla f(w_{t-1})\big),\] <p>where \(\alpha&gt;0\) is a step size (<em>aka</em> learning rate). An equivalent way of writing this (which will become useful later on) is<d-footnote>This can be proven using the definition of the proximal operator and completing the square.</d-footnote></p> \[\tag{1} w_{t} = \mathrm{argmin}_y \langle y-w_{t-1}, \nabla f(w_{t-1})\rangle + \varphi(y) + \frac{1}{2\alpha}\|y-w_{t-1}\|^2.\] <h3 id="weight-decay-as-a-proximal-operator">Weight decay as a proximal operator</h3> <p>For \(\ell_2\)-regularization \(\varphi(w) = \frac{\lambda}{2}\|w\|^2\), the proximal operator at \(w\) is given by \(\frac{1}{1+\lambda}w = (1-\frac{\lambda}{1+\lambda})w\). Based on this, the authors of <d-cite key="Zhuang2022"></d-cite> propose a proximal version of <span style="font-family:monospace">Adam</span> called <span style="font-family:monospace">ProxAdam</span>. It is given by</p> \[\tag{ProxAdam} w_t = \big(1- \frac{\lambda\eta_t}{1+\lambda\eta_t} \big)w_{t-1} - \frac{\eta_t \alpha}{1+\lambda\eta_t} \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}.\] <p>Knowing this, we can now understand why <span style="font-family:monospace">AdamW</span> is approximately a proximal version of <span style="font-family:monospace">Adam</span>. Using the first-order Taylor-approximation \(\frac{ax}{1+bx}\approx ax\) for small \(x\), applied to the coefficients in front of \(w_{t-1}\) and \(\frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\) gives the formula</p> \[w_t = (1-\eta_t \lambda)w_{t-1} - \eta_t \alpha \frac{\hat m_t}{\epsilon + \sqrt{\hat v_t}}\] <p>which is equal to <span style="font-family:monospace">AdamW</span>. The argument we just presented is exactly how <d-cite key="Zhuang2022"></d-cite> concludes that <span style="font-family:monospace">AdamW</span> \(\approx\) <span style="font-family:monospace">ProxAdam</span>.</p> <h3 id="changing-the-norm">Changing the norm</h3> <p>There is one more way of interpreting proximal methods. Let us begin with a simple example: Define the diagonal matrix \(D_t := \mathrm{Diag}(\epsilon + \sqrt{\hat v_t})\). Then, the <span style="font-family:monospace">Adam</span> update can be equivalently written<d-footnote>This can be proven by first-order optimality and solving for $w_t$. We will do a similar calculation further below.</d-footnote> as</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In other words, <span style="font-family:monospace">Adam</span> takes a proximal step of a linear function, but with the adaptive norm \(D_t\). This change in norm is what makes <span style="font-family:monospace">Adam</span> different from <span style="font-family:monospace">SGD</span> with (heavy-ball) momentum.</p> <p>The update formula of <span style="font-family:monospace">ProxAdam</span> can also be written as a proximal method:</p> \[\tag{P1} w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2\alpha}\|y\|_{D_t}^2 + \frac{1}{2 \eta_t \alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>In fact, the first-order optimality conditions of (P1) are</p> \[0 = \hat m_t + \frac{\lambda}{\alpha} D_t w_t + \frac{1}{\eta_t \alpha}D_t (w_t-w_{t-1}).\] <p>Solving for \(w_t\) (and doing simple algebra) gives</p> \[\tag{2} w_t = (1+\lambda \eta_t)^{-1}\big[w_{t-1} - \eta_t \alpha D_t^{-1} \hat m_t\big]\] <p>which is equal to <span style="font-family:monospace">ProxAdam</span>.</p> <p>What is slightly surprising here is the term \(\alpha^{-1}\|y\|_{D_t}^2\) in (P1) - we might have expected the regularization term to be used with the standard \(\ell_2\)-norm. This leads us to our final section.</p> <h2 id="adamw-is-scale-free"><span style="font-family:monospace">AdamW</span> is scale-free</h2> <p>As an alternative to (P1), we could replace \(\alpha^{-1}\|y\|_{D_t}^2\) by \(\|y\|^2\) and update</p> \[w_t = \mathrm{argmin}_y \langle y-w_{t-1}, \hat m_t \rangle + \frac{\lambda}{2}\|y\|^2 + \frac{1}{2\eta_t\alpha}\|y-w_{t-1}\|_{D_t}^2.\] <p>Again, setting the gradient of the objective to zero and solving for \(w_t\) we get</p> \[w_t = \big(\mathrm{Id} + \eta_t \lambda \alpha D_t^{-1}\big)^{-1} \big[w_{t-1} - \eta_t\alpha D_t^{-1} \hat m_t \big].\] <p>Comparing this to (2) we see that the second factor is the same, but the decay factor now also depends on \(D_t\) and \(\alpha\). Let us call this method <span style="font-family:monospace">AdamP</span>.</p> <p>Now the natural question is whether <span style="font-family:monospace">AdamP</span> or <span style="font-family:monospace">ProxAdam</span> (or <span style="font-family:monospace">AdamW</span> as its approximation) would be superior. One answer to this is that we would prefer a <em>scale-free</em> algorithm: with this we mean that if the loss function would be multiplied by a positive constant, we could still run the method with exactly the same parameters and obtain the same result. <span style="font-family:monospace">Adam</span> for example is scale-free and in <d-cite key="Zhuang2022"></d-cite> it is explained that <span style="font-family:monospace">ProxAdam</span>/<span style="font-family:monospace">AdamW</span> are, too. The reason for this is the following: looking at (P1) we see that if the loss is scaled by \(c&gt;0\), then \(\hat m_t\) and \(D_t\) are scaled by \(c\) (if we neglect the \(\epsilon\) in \(D_t\)). Hence, the objective in (P1) is multiplied by \(c\) which implies that <span style="font-family:monospace">ProxAdam</span> for \(\epsilon=0\) is invariant to scaling for the same values of \(\lambda,\alpha,\eta_t\). Now, for (P2) the story is different, as here the second term \(\frac{\lambda}{2}\|y\|^2\) is not scaled by \(c\), but the other terms are. We would need to rescale \(\lambda\) by \(c\) to obtain the identical update. As a consequence, <span style="font-family:monospace">AdamP</span> would <strong>not be scale-free</strong> and this makes it less attractive as a method. We should point out that scale-freeness is rather a practical advantage that requires less tuning when changing the model or dataset - it does not imply that the test accuracy would be different when both methods are tuned.</p> <p>To verify this, we ran a simple experiment on a ResNet20 for CIFAR10 with <code class="language-plaintext highlighter-rouge">(BN)</code> deactivated. For <span style="font-family:monospace">AdamW</span> (the <code class="language-plaintext highlighter-rouge">Pytorch</code> version) and <span style="font-family:monospace">AdamP</span> we tested the learning rates <code class="language-plaintext highlighter-rouge">[1e-3,1e-2,1e-1]</code> and weight decay <code class="language-plaintext highlighter-rouge">[1e-5,1e-4,1e-3,1e-2]</code>. From the plots below, we can see that both methods approximately achieve the same accuracy for the best configurations<d-footnote>The best configurations all have learning rate 1e-3.</d-footnote>. The only difference - in this very simple example - is that <span style="font-family:monospace">AdamP</span> seems to arrive at a model with smaller norm for the configurations with high accuracy (see right plot). Hence, its regularization seems to be stronger.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20val_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20val_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20val_score-1400.webp"/> <img src="/staging/assets/img/2022-12-01-adamw/resnet20val_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20model_norm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20model_norm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-adamw/resnet20model_norm-1400.webp"/> <img src="/staging/assets/img/2022-12-01-adamw/resnet20model_norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For the sake of completeness, we also add a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span> in the <a href="#appendix">Appendix</a>.</p> <h2 id="summary">Summary</h2> <ul> <li> <p>Weight decay can be seen as a proximal way of handling \(\ell_2\)-regularization. Therefore, it is not a different <em>type</em> of regularization itself but rather a different <em>treatment</em> of regularization in the optimization method. As a consequence, <span style="font-family:monospace">AdamW</span> is an (almost) proximal version of <span style="font-family:monospace">Adam</span>.</p> </li> <li> <p>Whether or not weight decay brings advantages when used <em>together with</em> <code class="language-plaintext highlighter-rouge">(BN)</code> seems to depend on several factors of the model and experimental design. However, in all experiments we discussed here <span style="font-family:monospace">AdamW</span> performed better or at least on par to <span style="font-family:monospace">AdamL2</span>.</p> </li> <li> <p>The second conclusion suggests that proximal algorithms such as <span style="font-family:monospace">AdamW</span> seem to be favourable. Together with the scale-free property that we described in the final section, this makes <span style="font-family:monospace">AdamW</span> a robust method and explains its practical success.</p> </li> </ul> <p><a name="appendix"></a></p> <h2 id="appendix">Appendix</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-adamw/krogh_snippet-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-adamw/krogh_snippet-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-adamw/krogh_snippet-1400.webp"/> <img src="/staging/assets/img/2022-12-01-adamw/krogh_snippet.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig. 2: Excerpt of the introduction in <d-cite key="Krogh1991"></d-cite>. </div> <p>Below you find a <code class="language-plaintext highlighter-rouge">Pytorch</code> implementation of <span style="font-family:monospace">AdamP</span>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>


<span class="k">class</span> <span class="nc">AdamP</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="s">"""
    Arguments:
        params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
        lr (float, optional): learning rate (default: 1e-3)
        betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.9, 0.999))
        eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
        
    """</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Invalid learning rate: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Invalid epsilon value: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Invalid beta parameter at index 0: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Invalid beta parameter at index 1: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">weight_decay</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"Invalid weight_decay value: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">_init_lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="k">return</span>
   

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="s">"""Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """</span>
        
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s">'params'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization
</span>                <span class="k">if</span> <span class="s">'step'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>
                    <span class="c1"># Exponential moving average of squared gradient values
</span>                    <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg_sq'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>
                    
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg'</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s">'exp_avg_sq'</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'betas'</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">state</span><span class="p">[</span><span class="s">'step'</span><span class="p">]</span>

                
                <span class="c1"># Decay the first and second moment running average coefficient
</span>                <span class="n">exp_avg</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="p">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">).</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span>
                <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="p">.</span><span class="n">div</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)).</span><span class="n">sqrt</span><span class="p">().</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s">'eps'</span><span class="p">])</span>

                <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span>
                <span class="n">lmbda</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s">'weight_decay'</span><span class="p">]</span>

                <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">lr</span><span class="o">/</span><span class="n">bias_correction1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">lmbda</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">div_</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">lr</span><span class="o">*</span><span class="n">lmbda</span><span class="o">/</span><span class="n">D</span><span class="p">)</span> <span class="c1"># adaptive weight decay
</span>
            

        <span class="k">return</span> <span class="n">loss</span></code></pre></figure>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Weight decay is among the most important tuning parameters to reach high accuracy for large-scale machine learning models. In this blog post, we revisit AdamW, the weight decay version of Adam, summarizing empirical findings as well as theoretical motivations from an optimization perspective.]]></summary></entry><entry><title type="html">Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/classification-layer-initialization-in-maml/" rel="alternate" type="text/html" title="Strategies for Classification Layer Initialization in Model-Agnostic Meta-Learning"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/classification-layer-initialization-in-maml</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/classification-layer-initialization-in-maml/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In a previous study, Raghu et al. [2020] <d-cite key="DBLP:conf/iclr/RaghuRBV20"></d-cite> found that in model-agnostic meta-learning (MAML) for few-shot classification, the majority of changes observed in the network during the inner loop fine-tuning process occurred in the linear classification head. It is commonly believed that during this phase, the linear head remaps encoded features to the classes of the new task. In traditional MAML, the weights of the final linear layer are meta-learned in the usual way. However, there are some issues with this approach:</p> <p>First, it is difficult to imagine that a single set of optimal weights can be learned. This becomes apparent when considering class label permutations: two different tasks may have the same classes but in a different order. As a result, the weights that perform well for the first task will likely not be effective for the second task. This is reflected in the fact that MAML’s performance can vary by up to 15% depending on the class label assignments during testing <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>.</p> <p>Second, more challenging datasets are being proposed as few-shot learning benchmarks, such as Meta-Dataset <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite>. These datasets have varying numbers of classes per task, making it impossible to learn a single set of weights for the classification layer.</p> <p>Therefore, it seems logical to consider how to initialize the final classification layer before fine-tuning on a new task. Random initialization may not be optimal, as it can introduce unnecessary noise <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>.</p> <p>This blog post will discuss different approaches to last layer initialization that claim to outperform the original MAML method.</p> <h2 id="quick-recap-on-maml">Quick recap on MAML</h2> <p>Model-Agnostic Meta-Learning (MAML) <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite> is a well-established algorithm in the field of optimization-based meta-learning. Its goal is to find parameters $\theta$ for a parametric model $f_{\theta}$ that can be efficiently adapted to perform an unseen task from the same task distribution, using only a few training examples. The pre-training of $\theta$ is done using two nested loops (bi-level optimization), with meta-training occurring in the outer loop and task-specific fine-tuning in the inner loop. The task-specific fine-tuning is typically done using a few steps of gradient descent:</p> \[\theta_{i}' = \theta - \alpha\nabla_{\theta}\mathcal{L_{\mathcal{T_{i}}}}(\theta, \mathcal{D^{tr}})\] <p>where $\alpha$ is the inner loop learning rate, $\mathcal{L_{\mathcal{T_{i}}}}$ is a task’s loss function, and $\mathcal{D^{tr}}$ is a task’s training set. The task includes a test set as well: $\mathcal{T_{i}} = (\mathcal{D_{i}^{tr}}, \mathcal{D_{i}^{test}})$.</p> <p>In the outer loop, the meta parameter $\theta$ is updated by backpropagating through the inner loop to reduce errors made on the tasks’ test set using the fine-tuned parameters:</p> \[\theta' = \theta - \eta\nabla_{\theta} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{} \mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}', \mathcal{D^{test}}).\] <p>Here, $\eta$ is the meta-learning rate. The differentiation through the inner loop involves calculating second-order derivatives, which mainly distinguishes MAML from simply optimizing for a $\theta$ that minimizes the average task loss.</p> <p>It is worth noting that in practical scenarios, this second-order differentiation is computationally expensive, and approximation methods such as first-order MAML (FOMAML) <d-cite key="DBLP:conf/icml/FinnAL17"></d-cite> or Reptile <d-cite key="DBLP:journals/corr/abs-1803-02999"></d-cite> are often used. In FOMAML, the outer loop update is simply: \(\theta' = \theta - \eta\nabla_{\theta'} \sum_{\mathcal{T_{i}} \sim p(\mathcal{T})}^{}\mathcal{L_{\mathcal{T_{i}}}}(\theta_{i}', \mathcal{D^{test}})\), which avoids differentiating through the inner loop.</p> <p>Before proceeding, let’s prepare ourselves for the next sections by looking at the notation we can use when discussing MAML in the few-shot classification regime: The model’s output prediction can be described as $\hat{y} = f_{\theta}(\mathbf{x}) = \underset{c\in[N]}{\mathrm{argmax}} ; h_{\mathbf{w}} (g_{\phi}(\mathbf{x}), c)$, where we divide our model $f_{\theta}(\mathbf{x})$ (which takes an input $\mathbf{x}$) into a feature extractor $g_{\phi}(\mathbf{x})$ and a classifier $h_\mathbf{w}(\mathbf{r}, c)$, which is parameterized by classification head weight vectors ${\mathbf{w}}_{c=1}^N$. $\mathbf{r}$ denotes an input’s representation, and $c$ is the index of the class we want the output prediction for.</p> <p>Finally, $\theta = {\mathbf{w_1}, \mathbf{w_1}, …, \mathbf{w_N}, \phi}$, and we are consistent with our previous notation.</p> <h2 id="learning-a-single-initialization-vector">Learning a single initialization vector</h2> <p>The first two variants of MAML - we look at - approach the initialization task by initializing the classification head weight vectors uniformly for all classes. In the paper</p> <p></p> <p><span>   ▶  </span>Han-Jia Ye &amp; Wei-Lun Chao (ICLR, 2022) How to train your MAML to excel in few-shot classification <d-cite key="DBLP:conf/iclr/YeC22"></d-cite>,</p> <p></p> <p>an approach called <strong>UnicornMAML</strong> is presented. It is explicitly motivated by the effect that different class-label assignments can have. Ye &amp; Chao [2022] <d-cite key="DBLP:conf/iclr/YeC22"></d-cite> report that during testing, vanilla MAML can perform very differently for <ins>tasks with the same set of classes</ins>, which are just <ins>differently ordered</ins>. Namely, they report that classification accuracy can vary up to 15% in the one-shot setting and up to 8% in the five-shot setting. This makes MAMLs performance quite unstable. <br/><br/></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final-1400.webp"/> <img src="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/perm_final.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <em>Fig.1 Example of MAML and a class label permutation. We can see the randomness introduced, as $\mathbf{w_1}$ is supposed to interpret the input features as "unicorn" for the first task, and as "bee" for the second. For both tasks, the class outputted as a prediction should be the same, as in human perception, both tasks are identical. This, however, is obviously not the case.</em> </p> <p>The solution proposed is fairly simple: Instead of meta-learning $N$ weight vectors for the final layer, only a <ins>single vector</ins> $\mathbf{w}$ is meta-learned and used to initialize all $ \{ \mathbf{w} \}_{c=1}^N $ before the fine-tuning stage.</p> <p>This forces the model to make random predictions before the inner loop, as $\hat{y_c}= h_{\mathbf{w}} (g_{\phi} (\mathbf{x}), c)$ will be the same for all $c \in [1,…,N ]$.</p> <p>After the inner loop, the updated parameters have been computed as usual: \(\theta' = \\{\mathbf{w_1}', \mathbf{w_2}', ..., \mathbf{w_N}', \phi'\\}\). The gradient for updating the single classification head meta weight vector $\mathbf{w}$, is just the aggregation of the gradients w.r.t. all the single $\mathbf{w_c}$:</p> \[\nabla_{\mathbf{w}} \mathcal{L_{\mathcal{T_i}}} (\mathcal{D^{test}}, \theta_i) = \sum_{c \in [N]} \nabla_{\mathbf{w_c}} \mathcal{L_{\mathcal{T_i}}} (\theta_i, \mathcal{D^{test}})\] <p>This collapses the models meta-parameters to $ \theta = \{\mathbf{w}, \phi\} $. <br/><br/></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final-1400.webp"/> <img src="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/unicorn_maml_final.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <em>Fig.2 Overview of UnicornMAML. We can see that class label permutations don't matter anymore, as before fine-tuning, the probability of predicting each class is the same.</em> </p> <p>This tweak to vanilla MAML makes UnicornMAML permutation invariant, as models fine-tuned on tasks including the same categories - just differently ordered - will now yield the same output predictions. Also, the method could be used with datasets where the number of classes varies without any further adaptation: It doesn’t matter how many classification head weight vectors are initialized by the single meta-classification head weight vector.</p> <p>Furthermore, the uniform initialization in Unicorn-MAML addresses the problem of memorization overfitting <d-cite key="DBLP:conf/iclr/YinTZLF20"></d-cite>. The phenomenon describes a scenario where a single model can learn all the training tasks only from the test data in the outer loop. This leads to a model that learns to perform the training tasks but also to a model that doesn’t do any fine-tuning and thus fails to generalize to unseen tasks. Again, the uniform initialization of the classification head for all classes forces the model to adapt during fine-tuning and thus prevents memorization overfitting.</p> <p>The approach is reported to perform on par with recent few-shot algorithms.</p> <p>Let’s finally think of how to interpret UnicornMAML: When meta-learning only a single classification head vector, one could say that not a mapping from features to classes is tried to be learned any more, but a prioritization of features, which seemed to be more relevant for the classification decision across tasks, than others.</p> <h2 id="zero-initialization">Zero initialization</h2> <p>The second approach for a uniform initialization is proposed in the paper</p> <p></p> <p><span>   ▶  </span>Chia-Hsiang Kao et al. (ICLR, 2022) MAML is a Noisy Contrastive Learner in Classification <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>.</p> <p></p> <p>Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> modify the original MAML by setting the whole classification head to zero before each inner loop. They refer to this MAML-tweak as the <strong>zeroing trick</strong>.</p> <p>An overview of MAML with the zeroing trick is displayed below:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick-1400.webp"/> <img src="/staging/assets/img/2022-12-01-classification-layer-initialization-in-maml/zeroing_trick.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p align="center"> <em>Fig.3 MAML with the zeroing trick applied.</em> </p> <p>Note that $S_n$ and $Q_n$ refer to $\mathcal{D_{i}^{tr}}$ and $\mathcal{D_{i}^{test}}$ in this notation.</p> <p>Through applying the zero initialization, three of the problems addressed by UnicornMAML are solved as well:</p> <ul> <li>MAML with the zeroing trick applied leads to random predictions before fine-tuning. This solves the problem of class label assignment permutations during testing.</li> <li>Through the random predictions before fine-tuning, memorization overfitting is prevented as well.</li> <li>The zeroing trick makes MAML applicable for datasets with a varying number of classes per task.</li> </ul> <p>Interestingly, the motivation for applying the zeroing trick, stated by Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>, is entirely different. In general, Kao et al. [2022] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> want to unveil in what sense MAML encourages its models to learn general-purpose feature representations. They show that under some assumptions, there is a supervised contrastive learning (SCL) objective underlying MAML. In SCL, the label information is leveraged by pulling embeddings belonging to the same class closer together while increasing the embedding distances of samples from different classes <d-cite key="DBLP:conf/nips/KhoslaTWSTIMLK20"></d-cite>.</p> <p>More specifically, they show that the outer-loop update for the encoder follows a noisy SCL loss under the following assumptions:</p> <ol> <li>The encoder weights are frozen in the inner loop (EFIL assumption)</li> <li>There is only a single inner loop update step.<d-footnote>Note that FOMAML technically follows a noisy SCL loss without this assumption. However, when applying the zeroing trick, this assumption is needed again for stating that the encoder update is following an SCL loss</d-footnote></li> </ol> <p>A noisy SCL loss means that cases can occur where the loss forces the model to maximize similarities between embeddings from samples of different classes. The outer-loop encoder loss in this setting contains an “interference term” which causes the model to pull together embeddings from different tasks or to pull embeddings into a random direction, with the randomness being introduced by random initialization of the classification head. Those two phenomena are termed <em>cross-task interference</em> and <em>initialization interference</em>. Noise and interference in the loss vanish when applying the zeroing trick, and the outer-loop encoder loss turns into a proper SCL loss. Meaning that minimizing this loss forces embeddings of the same class/task together while pushing embeddings from the same task and different classes apart. A decent increase in performance is observed for MAML with the zeroing trick compared to vanilla MAML.</p> <p>Those findings are derived using a general formulation of MAML, with a cross-entropy loss, and the details are available in the paper <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite>. Also, a slightly simpler example is stated to give an intuition of MAMLs SCL properties. We will briefly summarize it in the following to share this intuition with you.</p> <h3 id="mamls-scl-intuition">MAMLs SCL Intuition</h3> <p>To get an intuition of how MAML relates to SCL, let’s look at the following setup: an N-way one-shot classification task using MAML with Mean Squared Error (MSE) between the one-hot encoded class label and the prediction of the model. Furthermore, the EFIL assumption is made, the zeroing trick is applied, only a single inner loop update step is used, and only a single task is sampled per batch.</p> <p>In this setting, the classification heads inner-loop update for a single datapoint looks like this:</p> \[\mathbf{w}' = \mathbf{w} - \alpha (-g_{\phi} (\mathbf{x}_{1}^{tr}) \mathbf{t}_{1}^{tr\top})\] <p>$\mathbf{t}_1^{tr}$ refers to the one-hot encoded class label belonging to $\mathbf{x}_1^{tr}$. In words, the features extracted for training example $\mathbf{x}_1^{tr}$ are added to column $\mathbf{w}_c$, with $c$ being the index of 1 in $\mathbf{t}_1^{tr}$. For multiple examples, the features of all training examples labeled with class $c$ are added to the $c^{th}$ column of $\mathbf{w}$.</p> <p>Now, for calculating the model’s output in the outer loop, the model computes the dot products of the columns \(\\{\mathbf{w} \\}_{c=1}^N\) and the encoded test examples \(g_{\phi}(\mathbf{x}_1^{test})\) (and takes a softmax afterward.) To match the one-hot encoded label as good as possible, the dot product has to be large when \(\mathbf{t}_1^{test}\) = \(1\) at index \(c\), and small otherwise. We can see that the loss enforces embedding similarity for features from the same classes while enforcing dissimilarity for embeddings from different classes, which fits the SCL objective.</p> <h2 id="initialization-using-prototypes">Initialization using prototypes</h2> <p>A more sophisticated approach for last-layer initialization in MAML is introduced in the paper</p> <p></p> <p><span>   ▶  </span>Eleni Triantafillou et al. (ICLR, 2020) Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> .</p> <p></p> <p>As one might guess from the name, <strong>Proto-MAML</strong> makes use of Prototypical Networks (PNs) for enhancing MAML. Opposite to the two initialization strategies presented above, Proto-MAML does not uniformly initialize the classification head weights before each inner loop for all classes. Instead, it calculates class-specific initialization vectors based on the training examples. This solves some of the problems mentioned earlier (see <a href="#conclusion--discussion">Conclusion &amp; Discussion</a>), but also it adds another type of logic to the classification layer.</p> <p>Let’s revise how PNs work when used for few-shot learning for understanding Proto-MAML afterward:</p> <p>Class prototypes \(\mathbf{c}_{c}\) are computed by averaging over train example embeddings of each class, created by a feature extractor \(g_{\phi}(\mathbf{x})\). For classifying a test example, a softmax over the distances (e.g., squared euclidean distance) between class prototypes \(\mathbf{c}_{c}\) and example embeddings \(g_{\phi}(\mathbf{x}^{test})\) is used, to generate probabilities for each class.</p> <p>When using the squared euclidean distance, the model’s output logits are expressed as:</p> \[\begin{align*} &amp;- \vert \vert g_{\phi}(\mathbf{x}) - \mathbf{c}_c \vert \vert^2 \\ =&amp; −g_{\phi}(\mathbf{\mathbf{x}})^{\top} g_{\phi}(\mathbf{x}) + 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \mathbf{c}_{c}^{\top} \mathbf{c}_{c} \\ =&amp; 2 \mathbf{c}_{c}^{\top} g_{\phi}(\mathbf{x}) − \vert \vert \mathbf{c}_{c} \vert \vert^2 + constant. \end{align*}\] <p>Note that the “test” superscripts on $\mathbf{x}$ are left out for clarity. \(−g_{\phi}(\mathbf{x})^{\top} g_{\phi}(\mathbf{x})\) is disregarded here, as it’s the same for all logits, and thus doesn’t affect the output probabilities. When inspecting the left-over equation, we can see that it now has the shape of a linear classifier. More specifically, a linear classifier with weight vectors \(\mathbf{w}_c = 2 \mathbf{c}_c^{\top}\) and biases \(b_c = \vert \vert \mathbf{c}_{c} \vert \vert^2\).</p> <p>Proceeding to Proto-MAML, Triantafillou et al. [2020] <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> adapt vanilla MAML by initializing the classification head using the prototype weights and biases, as just discussed. The initialization happens before the inner loop for each task, and the prototypes are computed by MAMLs own feature extractor. Afterward, the fine-tuning works as usual. Finally, when updating $\theta$ in the outer loop, the gradients flow also through the initialization of \(\mathbf{w}_c\) and \(b_c\), which is easy as they fully depend on \(g_{\phi}(\mathbf{x})\).</p> <p>Note that because of computational reasons, Triantafillou et al. [2020] <d-cite key="DBLP:conf/iclr/TriantafillouZD20"></d-cite> refer to Proto-MAML as (FO-)Proto-MAML.</p> <p>With Proto-MAML, one gets a task-specific, data-dependent initialization in a simple fashion, which seems super nice. For computing the model’s output logits after classification head initialization, dot products between class prototypes and embedded examples are computed, which again seems very reasonable.</p> <p>One could argue that in the one-shot scenario, Proto-MAML doesn’t learn that much in the inner loop beside the initialization itself. This happens as the dot product between an embedded training example and one class prototype (which equals the embedded training example itself for one class) will be disproportionately high. For a k-shot example, this effect might be less, but still, there is always one training example embedding within the prototype to compare. Following this thought, the training samples would rather provide a useful initialization of the final layer than a lot of parameter adaptation. One has to say that Proto-MAML performed quite well in the authors’ experiments.</p> <h2 id="what-else-is-there">What else is there?</h2> <p>Before proceeding to <a href="#conclusion--discussion">Conclusion &amp; Discussion</a>, here are some pointers to methods that did not perfectly fit the topic but which are closely related:</p> <p>The first method worth mentioning is called Latent Embedding Optimization (LEO) <d-cite key="DBLP:conf/iclr/RusuRSVPOH19"></d-cite>. The authors encode the training data in a low dimensional subspace, from which model parameters $\theta$ can be generated. In the example presented, $\theta$ consists only of $\mathbf{w}$, so for the first inner-loop iteration, this would perfectly fit our initialization topic. The low dimensional code is generated using a feed-forward encoder, as well as a matching network. Using the matching network allows LEO to consider relations between the training examples of different classes. Very similar classes, for example, might require different decision boundaries than more distinct classes, hence the intuition.</p> <p>LEO deviates from the initialization scheme, however, as optimization is done in the low dimensional subspace and not on the model’s parameters directly. It is stated that optimizing in a lower dimensional subspace helps in low-data regimes.</p> <p>Another related method is called MetaOptNet <d-cite key="DBLP:conf/cvpr/LeeMRS19"></d-cite>. In this approach, convex base learners, like support vector machines, are used as the classification head. Those can be optimized till convergence, which solves e.g., the problem of varying performance due to random class label assignments.</p> <h2 id="conclusion--discussion">Conclusion &amp; Discussion</h2> <p>To conclude, we’ve seen that a variety of problems can be tackled by using initialization strategies for MAMLs linear classification head, including:</p> <ul> <li>Varying performance due to random class label assignments</li> <li>Ability of MAML to work on datasets where the number of classes per task varies</li> <li>Memorization overfitting</li> <li>Cross-task interference</li> <li>and Initialization interference.</li> </ul> <p>Furthermore, for all the approaches presented, a decent gain in performance is reported in comparison to vanilla MAML. It seems, therefore, very reasonable to spend some time thinking about the last layer initialization.</p> <p>Looking at the problems mentioned and variants discussed in more detail, we can state that all the different variants make MAML <strong>permutation invariant with regard to class label assignments</strong>. UnicornMAML and the zeroing trick solve it by uniform initialization of $\mathbf{w}$. In Proto-MAML, the initialization happens with regard to the class label assignment, so it’s permutation invariant as well.</p> <p>Also, all variants are compatible with <strong>datasets where the number of classes per task varies</strong>. In UnicornMAML, an arbitrary number of classification head vectors can be initialized with the single meta-learned classification head weight vector. When zero-initializing the classification head, the number of classes per task does not matter as well. In Proto-MAML, prototypes can be computed for an arbitrary number of classes, so again, the algorithm works on such a dataset without further adaption.</p> <p>Next, UnicornMAML and the zeroing trick solve <strong>memorization overfitting</strong>, again by initializing $\mathbf{w}$ uniformly for all classes. Proto-MAML solves memorization overfitting as well, as the task-specific initialization of $\mathbf{w}$ itself can be interpreted as fine-tuning.</p> <p><strong>Cross-task interference</strong> and <strong>initialization interference</strong> are solved by the zeroing trick. For the other models, this is harder to say, as the derivations made by <a href="#Kao">Kao et al.</a> are quite a case specific. Intuitively, Proto-MAML should solve cross-task interference, as the classification head is reinitialized after each task. Initialization interference is not solved by either ProtoMAML or UnicornMAML, as random initialization remains.</p> <p>Note that in discussion with a reviewer, <a href="#kao">Kao et al.</a> state that the main results they show are achieved by models which had the zeroing trick implemented but which didn’t follow the EFIL assumption. They argue that using only the zeroing trick still enhances supervised contrastiveness. This kind of puts their whole theory into perspective, as without the EFIL assumption, MAML with the zeroing trick is neither an SCL algorithm nor a noisy SCL algorithm. Still, noticeable performance gains are reported though.</p> <p>The question arises whether the whole theoretical background is needed or whether the zeroing tricks benefit is mainly the uniform initialization, like in UnicornMAML. It would be nice to see how the single learned initialization vector in UnicornMAML turns out to be shaped and how it compares to the zeroing trick. While the zeroing trick reduces cross-task noise and initialization noise, a single initialization vector can weight some features as more important than others for the final classification decision across tasks.</p> <p>In contrast to the uniform initialization approaches, we have seen Proto-MAML, where class-specific classification head vectors are computed for initialization based on the training data.</p> <p>Finally, Ye et al. [2022] <d-cite key="DBLP:conf/iclr/YeC22"></d-cite> compare the performance between Proto-MAML and UnicornMAML on MiniImageNet and TieredImageNet. UnicornMAML performs slightly better here in the one- and five-shot settings. Kao et al. [2020] <d-cite key="DBLP:conf/iclr/KaoCC22"></d-cite> do not report any particular numbers for their zeroing trick.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[["This blog post discusses different strategies for initializing the classification layers parameters before fine-tuning on a new task in Model-Agnostic Meta-Learning."]]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/distill-example/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/iclr-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2023-05-01-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/9-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/8-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/10-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/11-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/12-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/staging/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="n">write_html</span><span class="p">(</span><span class="s">'./assets/html/2022-12-01-distill-example/plotly_demo_1.html'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2022-12-01-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/staging/assets/html/2022-12-01-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1676101978692" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1676101978692 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1676101978692 .node circle,#mermaid-1676101978692 .node ellipse,#mermaid-1676101978692 .node polygon,#mermaid-1676101978692 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1676101978692 .node.clickable{cursor:pointer}#mermaid-1676101978692 .arrowheadPath{fill:#333}#mermaid-1676101978692 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1676101978692 .edgeLabel{background-color:#e8e8e8}#mermaid-1676101978692 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1676101978692 .cluster text{fill:#333}#mermaid-1676101978692 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1676101978692 .actor{stroke:#ccf;fill:#ececff}#mermaid-1676101978692 text.actor{fill:#000;stroke:none}#mermaid-1676101978692 .actor-line{stroke:grey}#mermaid-1676101978692 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1676101978692 .messageLine0,#mermaid-1676101978692 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1676101978692 #arrowhead{fill:#333}#mermaid-1676101978692 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1676101978692 .messageText{fill:#333;stroke:none}#mermaid-1676101978692 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1676101978692 .labelText,#mermaid-1676101978692 .loopText{fill:#000;stroke:none}#mermaid-1676101978692 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1676101978692 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1676101978692 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1676101978692 .section{stroke:none;opacity:.2}#mermaid-1676101978692 .section0{fill:rgba(102,102,255,.49)}#mermaid-1676101978692 .section2{fill:#fff400}#mermaid-1676101978692 .section1,#mermaid-1676101978692 .section3{fill:#fff;opacity:.2}#mermaid-1676101978692 .sectionTitle0,#mermaid-1676101978692 .sectionTitle1,#mermaid-1676101978692 .sectionTitle2,#mermaid-1676101978692 .sectionTitle3{fill:#333}#mermaid-1676101978692 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1676101978692 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1676101978692 .grid path{stroke-width:0}#mermaid-1676101978692 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1676101978692 .task{stroke-width:2}#mermaid-1676101978692 .taskText{text-anchor:middle;font-size:11px}#mermaid-1676101978692 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1676101978692 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1676101978692 .taskText0,#mermaid-1676101978692 .taskText1,#mermaid-1676101978692 .taskText2,#mermaid-1676101978692 .taskText3{fill:#fff}#mermaid-1676101978692 .task0,#mermaid-1676101978692 .task1,#mermaid-1676101978692 .task2,#mermaid-1676101978692 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1676101978692 .taskTextOutside0,#mermaid-1676101978692 .taskTextOutside1,#mermaid-1676101978692 .taskTextOutside2,#mermaid-1676101978692 .taskTextOutside3{fill:#000}#mermaid-1676101978692 .active0,#mermaid-1676101978692 .active1,#mermaid-1676101978692 .active2,#mermaid-1676101978692 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1676101978692 .activeText0,#mermaid-1676101978692 .activeText1,#mermaid-1676101978692 .activeText2,#mermaid-1676101978692 .activeText3{fill:#000!important}#mermaid-1676101978692 .done0,#mermaid-1676101978692 .done1,#mermaid-1676101978692 .done2,#mermaid-1676101978692 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1676101978692 .doneText0,#mermaid-1676101978692 .doneText1,#mermaid-1676101978692 .doneText2,#mermaid-1676101978692 .doneText3{fill:#000!important}#mermaid-1676101978692 .crit0,#mermaid-1676101978692 .crit1,#mermaid-1676101978692 .crit2,#mermaid-1676101978692 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1676101978692 .activeCrit0,#mermaid-1676101978692 .activeCrit1,#mermaid-1676101978692 .activeCrit2,#mermaid-1676101978692 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1676101978692 .doneCrit0,#mermaid-1676101978692 .doneCrit1,#mermaid-1676101978692 .doneCrit2,#mermaid-1676101978692 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1676101978692 .activeCritText0,#mermaid-1676101978692 .activeCritText1,#mermaid-1676101978692 .activeCritText2,#mermaid-1676101978692 .activeCritText3,#mermaid-1676101978692 .doneCritText0,#mermaid-1676101978692 .doneCritText1,#mermaid-1676101978692 .doneCritText2,#mermaid-1676101978692 .doneCritText3{fill:#000!important}#mermaid-1676101978692 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1676101978692 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1676101978692 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1676101978692 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1676101978692 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1676101978692 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1676101978692 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1676101978692 #compositionEnd,#mermaid-1676101978692 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1676101978692 #aggregationEnd,#mermaid-1676101978692 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1676101978692 #dependencyEnd,#mermaid-1676101978692 #dependencyStart,#mermaid-1676101978692 #extensionEnd,#mermaid-1676101978692 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1676101978692 .branch-label,#mermaid-1676101978692 .commit-id,#mermaid-1676101978692 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1676101978692{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nx">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.]]></summary></entry><entry><title type="html">A Hitchhiker’s Guide to Momentum</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/hitchhikers-momentum/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Momentum"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/hitchhikers-momentum</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/hitchhikers-momentum/"><![CDATA[ <div style="display: none"> $$ \def\argmin{\mathop{\mathrm{arg\,min}}} \def\xx{\pmb{x}} \def\HH{\pmb{H}} \def\bb{\pmb{b}} \def\EE{ \mathbb{E} } \def\RR{ \mathbb{R} } \def\lmax{L} \def\lmin{\mu} \def\defas{\stackrel{\text{def}}{=}} \definecolor{colormomentum}{RGB}{27, 158, 119} \definecolor{colorstepsize}{RGB}{217, 95, 2} \def\mom{ {\color{colormomentum}{m}} } \def\step{ {\color{colorstepsize}h} } $$ </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2> <p>Gradient descent with momentum,<d-cite key="polyak1964some"></d-cite> also known as heavy ball or momentum for short, is an optimization method designed to solve unconstrained minimization problems of the form \begin{equation} \argmin_{\xx \in \RR^d} \, f(\xx)\,, \end{equation} where the objective function \(f\) is differentiable and we have access to its gradient \(\nabla f\). In this method the update is a sum of two terms. The first term is the difference between the current and the previous iterate \((\xx_{t} - \xx_{t-1})\), also known as <em>momentum term</em>. The second term is the gradient \(\nabla f(\xx_t)\) of the objective function.</p> <p class="framed"> <b class="underline">Gradient Descent with Momentum</b><br/> <b>Input</b>: starting guess \(\xx_0\), step-size \(\step &gt; 0\) and momentum parameter \(\mom \in (0, 1)\).<br/> \(\xx_1 = \xx_0 - \dfrac{\step}{\mom+1} \nabla f(\xx_0)\) <br/> <b>For</b> \(t=1, 2, \ldots\) compute \begin{equation}\label{eq:momentum_update} \xx_{t+1} = \xx_t + \mom(\xx_{t} - \xx_{t-1}) - \step\nabla f(\xx_t) \end{equation} </p> <p>Despite its simplicity, gradient descent with momentum exhibits unexpectedly rich dynamics that we’ll explore on this post.</p> <p>The origins of momentum can be traced back to Frankel’s method in the 1950s for solving linear system of equations.<d-cite key="frankel1950convergence"></d-cite> It was later generalized by Boris Polayk to non-quadratic objectives<d-cite key="polyak1964some"></d-cite>. In recent years there has been a resurgence in interest in this venerable method, as a stochastic variant of this method, where the gradient is replaced by a stochastic estimate, is one of the most popular methods for deep learning. This has led in recent years to a flurry fo research –and improved understanding – of this stochastic variant. Although this blog posts limits itself with the deterministic variant, the interested reader is encouraged to explore following references. A good starting point is the paper by <a href="https://arxiv.org/abs/1712.07628">Sutskever et al.</a>,<d-cite key="sutskever2013importance"></d-cite> which was among the firsts to highlight the importance of momentum for deep learning optimization. More recent progress include an analysis of the last iteration of the method by <a href="https://arxiv.org/abs/2104.09864">Tao et al.</a><d-cite key="tao2021the"></d-cite> and a paper by <a href="https://arxiv.org/abs/2106.07587">Liu et al.</a><d-cite key="Liu2020Accelerating"></d-cite> that develops accelerated variants for over-parameterized models.</p> <p>Coming back to the subject of our post, the (non-stochastic) gradient descendent with momentum method, a paper that also explores the dynamics of momentum is Gabriel Goh’s <a href="https://distill.pub/2017/momentum/">Why Momentum Really Works</a>.<d-cite key="goh2017momentum"></d-cite> There are subtle but important differences between both analysis. The landscape described in the section <a href="https://distill.pub/2017/momentum/#momentum2D">“The Dynamics of Momentum”</a> describe the improvement along the direction <em>of a single eigenvector</em>. This partial view produces some misleading conclusions. For example, along the direction of a single eigenvector, the largest improvement is achieved with zero momentum and a step-size of 1 over the associated eigenvalue. This conclusion however doesn’t hold in higher dimensions, where as we will see, the momentum term that yields the fastest convergence is non-zero.</p> <h2 id="how-fast-is-momentum">How fast is Momentum?</h2> <p>Momentum is <em>fast</em>. So fast that it’s often the default choice of machine learning practitioners. But can we quantify this more precisely?</p> <p>Throughout the post we’ll assume that our objective function \(f\) is a quadratic objective of the form \begin{equation}\label{eq:opt} f(\xx) \defas \frac{1}{2}(\xx - \xx^\star) \HH (\xx - \xx^\star)~, \end{equation} where \(\HH\) is a symmetric positive definite matrix and \(\xx^\star\) is the minimizer of the objective. We’ll assume that the eigenvalues of \(\HH\) are in the interval \([\mu, L]\).</p> <p>The measure we’ll use to quantify the speed of convergence is the rate of convergence. This is the worst-case relative improvement in the iterate suboptimality at iteration \(t\), defined as \begin{equation}\label{eq:convergence_rate} r_t \defas \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|\xx_{t} - \xx^\star\|}{\|\xx_{0} - \xx^\star\|}\,. \end{equation} This is a worst-case measure because of all problem instances, we take worst possible initialization \(\xx_0\) and matrix \(\HH\) with eigenvalues in the interval \([\mu, L]\).</p> <p>This is a measure of how much progress is made (in the worst-case) at iteration \(t\). The smaller the value of \(r_t\), the faster the algorithm converges. Since all algorithms that we consider converge exponentially fast, for large enough \(t\) the error is of the order of \(\mathcal{O}{(\text{constant}^t)}\). Hence the most informative quantity is the value of \(\text{constant}\) in this expression. We’ll call this quantity the <i>asymptotic rate of convergence</i>, and denote it: \begin{equation} r_{\infty} \defas \limsup_{t \to \infty} \sqrt[t]{r_t}\,. \end{equation} This is the quantity we’ll be discussing throughout the post and what we’ll use to compare the speed of momentum for different values of its hyperparameters.</p> <h3 id="a-connection-between-optimization-methods-and-polynomials">A connection between optimization methods and polynomials</h3> <p>To compute easily the asymptotic rate of convergence for all admissible values of step-size and momentum, we’ll use a connection between optimization of quadratic functions and the theory of orthogonal polynomials. This theory was extensively used in the early days of numerical analysis <d-cite key="Rutishauser1959"></d-cite> and provides an elegant and simple way to compute asymptotic rates (and non-asymptotic ones, althought not the topic of this blog post) from known results in the theory of orthogonal polynomials. We favor this technique for its simplicity and elegance, although ones ones that also be used with identical results. Other techniques include the linear operator technique used by Polyak,<d-cite key="polyak1964some"></d-cite> the estimate sequences technique pioneered by Nesterov<d-cite key="nesterov1983method"></d-cite> or the use of Lyapunov functions.<d-cite key="JMLR:v22:20-195"></d-cite></p> <p>The main result that will allow us to make the link between optimization and orthogonal polynomials is the following result. It’s origins seem unclear, although a proof can be found in the 1959 monograph of Rutishauser.<d-cite key="Rutishauser1959"></d-cite></p> <p class="lemma"> Consider the following polynomial \(P_t\) of degree \(t\), defined recursively as: \begin{equation} \begin{split} &amp;P_{t+1}(\lambda) = (1 + \mom - \step \lambda ) P_{t}(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;P_1(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, ~ P_0(\lambda) = 1\,,~ \end{split}\label{eq:def_residual_polynomial2} \end{equation} Then we can write the suboptimality at iteration \(t\) as \begin{equation} \xx_t - \xx^\star = P_t(\HH) \left( \xx_0 - \xx^\star \right) \,, \end{equation} where \(P_t(\HH)\) is the matrix obtained from evaluating the (originally rel-valued) polynomial \(P_t\) at the matrix \(\HH\). </p> <p>This last identity will allow us to easily compute convergence rates. In particular, plugging it into the definition of the convergence rate \eqref{eq:convergence_rate} we get that the rate is determined by the absolute value of the residual polynomial over the \([\mu, L]\) interval: \begin{align} r_t &amp;= \sup_{\xx_0, \text{eigs}(\HH) \in [\mu, L]} \frac{\|P_t(\HH) \left( \xx_0 - \xx^\star \right)\|}{\|\xx_{0} - \xx^\star\|} \\ &amp; = \sup_{\text{eigs}(\HH) \in [\mu, L]} \|P_t(\HH)\| \\ &amp; = \sup_{\lambda \in [\mu, L]} \lvert P_t(\lambda) \rvert\,. \end{align} We’ve now reduced the problem of computing the convergence rate to the problem of computing the absolute value of a polynomial over a given interval. This is a problem that has been extensively studied in the theory of orthogonal polynomials. In particular, we’ll use known bounds on Chebyshev polynomials of the first and second kind, as the residual polynomial of momentum can be written as a convex combination of these two polynomials. This fact is proven in the next result, which is a generalization of equation (II.29) in (Rutishauser 1959).<d-cite key="Rutishauser1959"></d-cite></p> <p class="lemma"> The residual polynomial of momentum can be written in terms of Chebyshev polynomials of the first and second kind as \begin{align} P_t(\lambda) = \mom^{t/2} \left( {\small\frac{2\mom}{1+\mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lambda))\right)\,. \end{align} where \(\sigma(\lambda) = {\small\dfrac{1}{2\sqrt{\mom}}}(1 + \mom - \step\,\lambda)\,\) is a linear function that we'll refer to as the <span class="underline">link function</span> and \(T_t\) and \(U_t\) are the Chebyshev polynomials of the first and second kind respectively. </p> <div class="wrap-collapsible-XXX"> <input id="collapsible3" class="toggle" type="checkbox"/> <label for="collapsible3" class="lbl-toggle" tabindex="0"><b>Show proof</b></label><div class="collapsible-content"><div class="content-inner"><div class="proof" id="proof-variance"> <p> Let's denote by \(\widetilde{P}_t\) the right hand side of the above equation, that is, \begin{equation} \widetilde{P}_{t}(\lambda) \defas \mom^{t/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_t(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_t(\sigma(\lambda))\right)\,. \end{equation} Our goal is to show that \(P_t = \widetilde{P}_t\) for all \(t\). </p> <p> For \(t=1\), \(T_1(\lambda) = \lambda\) and \(U_1(\lambda) = 2\lambda\), so we have \begin{align} \widetilde{P}_1(\lambda) &amp;= \sqrt{\mom} \left(\tfrac{2 \mom}{1 + \mom} \sigma(\lambda) + \tfrac{1 - \mom}{1 + \mom} 2 \sigma(\lambda)\right)\\ &amp;= \frac{2 \sqrt{\mom}}{1 + \mom} \sigma(\lambda) = 1 - \frac{\step}{1 + \mom} \lambda\,, \end{align} which corresponds to the definition of \(P_1\) in \eqref{eq:def_residual_polynomial2}. </p> <p> Assume it's true for any iteration up to \(t\), we will show it's true for \(t+1\). Using the three-term recurrence of Chebyshev polynomials we have \begin{align} &amp;\widetilde{P}_{t+1}(\lambda) = \mom^{(t+1)/2} \left( {\small\frac{2 \mom}{1 + \mom}}\, T_{t+1}(\sigma(\lambda)) + {\small\frac{1 - \mom}{1 + \mom}}\, U_{t+1}(\sigma(\lambda))\right) \\ &amp;= \mom^{(t+1)/2} \Big( {\small\frac{2 \mom}{1 + \mom}}\, (2 \sigma(\lambda) T_{t}(\sigma(\lambda)) - T_{t-1}(\sigma(\lambda))) \nonumber\\ &amp;\qquad\qquad + {\small\frac{1 - \mom}{1 + \mom}}\, (2 \sigma(\lambda) U_{t}(\sigma(\lambda)) - U_{t-1}(\sigma(\lambda)))\Big)\\ &amp;= 2 \sigma(\lambda) \sqrt{\mom} P_t(\lambda) - \mom P_{t-1}(\lambda)\\ &amp;= (1 + \mom - \step \lambda) P_t(\lambda) - \mom P_{t-1}(\lambda) \end{align} where the third identity follows from grouping polynomials of same degree and the induction hypothesis. The last expression is the recursive definition of \(P_{t+1}\) in \eqref{eq:def_residual_polynomial2}, which proves the desired \(\widetilde{P}_{t+1} = {P}_{t+1}\). </p> </div></div></div></div> <h3 id="tools-of-the-trade-the-two-faces-of-chebyshev-polynomials">Tools of the trade: the two faces of Chebyshev polynomials</h3> <p>A key feature that we’ll use extensively about Chebyshev polynomials is that they behave very differently inside and outside the interval \([-1, 1]\). Inside this interval (shaded blue region) the magnitude of these polynomials stays close to zero, while outside it explodes:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/two_phases_chebyshev.gif" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s make this observation more precise.</p> <p><strong>Inside</strong> the \([-1, 1]\) interval, Chebyshev polynomials admit the <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials#Trigonometric_definition">trigonometric definitions</a> \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_{t}(\cos(\theta)) = \sin((t+1)\theta) / \sin(\theta)\) and so they have an oscillatory behavior with values bounded in absolute value by 1 and \(t+1\) respectively.</p> <p><strong>Outside</strong> of this interval the Chebyshev polynomials of the first kind admit the <a href="https://en.wikipedia.org/wiki/Chebyshev_polynomials#Explicit_expressions">explicit form</a> for \(|\xi| \ge 1\): \begin{align} T_t(\xi) &amp;= \dfrac{1}{2} \Big(\xi-\sqrt{\xi^2-1} \Big)^t + \dfrac{1}{2} \Big(\xi+\sqrt{\xi^2-1} \Big)^t \\ U_t(\xi) &amp;= \frac{(\xi + \sqrt{\xi^2 - 1})^{t+1} - (\xi - \sqrt{\xi^2 - 1})^{t+1}}{2 \sqrt{\xi^2 - 1}}\,. \end{align} We’re interested in convergence rates, so we’ll look into \(t\)-th root asymptotics of the quantities.<d-footnote>With little extra effort, it would be possible to derive non-asymptotic convergence rates, although I won't pursue this analysis here.</d-footnote> Luckily, these asymptotics are the same for both polynomials<d-footnote>Although we won't use it here, this \(t\)-th root asymptotic holds for (almost) all orthogonal polynomials, not just Chebyshev polynomials. See for instance reference below</d-footnote> <d-cite key="stahl1990nth"></d-cite> and taking limits we have that \begin{equation} \lim_{t \to \infty} \sqrt[t]{|T_t(\xi)|} = \lim_{t \to \infty} \sqrt[t]{|U_t(\xi)|} = |\xi| + \sqrt{\xi^2 - 1}\,. \end{equation}</p> <h2 id="the-robust-region">The Robust Region</h2> <p>Let’s start first by considering the case in which the image of \(\sigma\) is in the \([-1, 1]\) interval. This is the most favorable case. In this case, the Chebyshev polynomials are bounded in absolute value by 1 and \(t+1\) respectively. Since the Chebsyshev polynomials are evaluated at \(\sigma(\cdot)\), this implies that \(\lvert \sigma(\lambda)\rvert \leq 1\). We’ll call the set of step-size and momentum parameters for which the previous inequality is verified the <em>robust region</em>.</p> <p>Let’s visualize this region in a map. Since \(\sigma\) is a linear function, its extremal values are reached at the edges: \begin{equation} \max_{\lambda \in [\lmin, \lmax]} |\sigma(\lambda)| = \max{|\sigma(\lmin)|, |\sigma(\lmax)|}\,. \end{equation} Using \(\lmin \leq \lmax\) and that \(\sigma(\lambda)\) is decreasing in \(\lambda\), we can simplify the condition \(\lvert \sigma(\lambda)\rvert \leq 1\) to \(\sigma(\lmin) \leq 1\) and \(\sigma(L) \geq -1\), which in terms of the step-size and momentum correspond to: \begin{equation}\label{eq:robust_region} \frac{(1 - \sqrt{\mom})^2}{\lmin} \leq \step \leq \frac{(1 + \sqrt{\mom})^2}{L} \,. \end{equation} These two conditions provide the upper and lower bound of the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_robust_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate">Asymptotic rate</h3> <p>Let \(\sigma(\lambda) = \cos(\theta)\) for some \(\theta\), which is always possible since \(\sigma(\lambda) \in [-1, 1]\). In this regime, Chebyshev polynomials verify the identities \(T_t(\cos(\theta)) = \cos(t \theta)\) and \(U_t(\cos(\theta)) = \sin((t+1)\theta)/\sin(\theta)\) , which replacing in the definition of the residual polynomial gives \begin{equation} P_t(\sigma^{-1}(\cos(\theta))) = \mom^{t/2} \left[ {\small\frac{2\mom}{1+\mom}}\, \cos(t\theta) + {\small\frac{1 - \mom}{1 + \mom}}\,\frac{\sin((t+1)\theta)}{\sin(\theta)}\right]\,. \end{equation}</p> <p>Since the expression inside the square brackets is bounded in absolute value by \(t+2\), taking \(t\)-th root and then limits we have \(\limsup_{t \to \infty} \sqrt[t]{\lvert P_t(\sigma^{-1}(\cos(\theta)))\rvert} = \sqrt{\mom}\) for <i>any</i> \(\theta\). This gives our first asymptotic rate:</p> <p class="framed" style="text-align: center"> The asymptotic rate in the robust region is \(r_{\infty} = \sqrt{\mom}\). </p> <p>This is nothing short of magical. It would seem natural –and this will be the case in other regions– that the speed of convergence should depend on both the step-size and the momentum parameter. Yet, this result implies that it’s not the case in the robust region. In this region, the convergence <i>only</i> depends on the momentum parameter $\mom$. Amazing.<d-footnote>This insensitivity to step-size has been leveraged by Zhang et al. 2018 to develop a momentum tuner </d-footnote> <d-cite key="zhang2017yellowfin"></d-cite></p> <p>This also illustrates why we call this the <i>robust</i> region. In its interior, perturbing the step-size in a way that we stay within the region has no effect on the convergence rate. The next figure displays the asymptotic rate (darker is faster) in the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_robust_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="the-lazy-region">The Lazy Region</h2> <p>Let’s consider now what happens outside of the robust region. In this case, the convergence will depend on the largest of \(\{\lvert\sigma(\lmin)\rvert, \lvert\sigma(L)\rvert\}\). We’ll consider first the case in which the maximum is \(\lvert\sigma(\lmin)\rvert\) and leave the other one for next section.</p> <p>This region is determined by the inequalities \(\lvert\sigma(\lmin)\rvert &gt; 1\) and \(\lvert\sigma(\lmin)\rvert \geq \lvert\sigma(L)\rvert\). Using the definition of \(\sigma\) and solving for \(\step\) gives the equivalent conditions \begin{equation} \step \leq \frac{2(1 + \mom)}{L + \lmin} \quad \text{ and }\quad \step \leq \frac{(1 - \sqrt{\mom})^2}{\lmin}\,. \end{equation} Note the second inequality is the same one as for the robust region \eqref{eq:robust_region} but with the inequality sign reversed, and so the region will be on the oposite side of that curve. We’ll call this the <i>lazy region</i>, as in increasing the momentum will take us out of it and into the robust region.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_lazy_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate-1">Asymptotic rate</h3> <p>As we saw earlier, outside of the \([-1, 1]\) interval both Chebyshev have simple \(t\)-th root asymptotics. Using this and that both kinds of Chebyshev polynomials agree in sign outside of the \([-1, 1]\) interval we can compute the asymptotic rate as \begin{align} \lim_{t \to \infty} \sqrt[t]{r_t} &amp;= \sqrt{\mom} \lim_{t \to \infty} \sqrt[t]{ {\small\frac{2\mom}{\mom+1}}\, T_t(\sigma(\lmin)) + {\small\frac{1 - \mom}{1 + \mom}}\,U_t(\sigma(\lmin))} \\ &amp;= \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right) \\ \end{align} This gives the asymptotic rate for this region</p> <p class="framed" style="text-align: center"> In the lazy region the asymptotic rate is \(r_{\infty} = \sqrt{\mom}\left(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1} \right)\). </p> <p>Unlike in the robust region, this rate depends on both the step-size and the momentum parameter, which enters in the rate through the link function \(\sigma\). This can be observed in the color plot of the asymptotic rate</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_lazy_region.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="knifes-edge">Knife’s Edge</h2> <p>The robust and lazy region occupy most (but not all!) of the region for which momentum converges. There’s a small region that sits between the lazy and robust regions and the region where momentum diverges. We call this region the <i>Knife’s edge</i></p> <p>For parameters not in the robust or lazy region, we have that \(|\sigma(L)| &gt; 1\) and \(|\sigma(L)| &gt; |\sigma(\lmin)|\). Using the asymptotics of Chebyshev polynomials as we did in the previous section, we have that the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). The method will only converge when this asymptotic rate is below 1. Enforcing this results in \(\step \lt 2 (1 + \mom) / L\). Combining this condition with the one of not being in the robust or lazy region gives the characterization: \begin{equation} \step \lt \frac{2 (1 + \mom)}{L} \quad \text{ and } \quad \step \geq \max\Big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\Big\}\,. \end{equation}</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/sketch_knife_edge.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="asymptotic-rate-2">Asymptotic rate</h3> <p>The asymptotic rate can be computed using the same technique as in the lazy region. The resulting rate is the same as in that region but with \(\sigma(L)\) replacing \(\sigma(\lmin)\):</p> <p class="framed" style="text-align: center"> In the Knife's edge region the asymptotic rate is \(\sqrt{\mom}\left(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1} \right)\). </p> <p>Pictorially, this corresponds to</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_knife_edge.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="putting-it-all-together">Putting it All Together</h2> <p>This is the end of our journey. We’ve visited all the regions on which momentum converges.<d-footnote>There's a small convergent region with <i>negative</i> momentum parameter that we haven't visited. Although not typically used for minimization, negative momentum has found applications in smooth games <a href="https://arxiv.org/abs/1807.04740">(Gidel et al., 2020)</a>.</d-footnote> The only thing left to do is to combine all the asymptotic rates we’ve gathered along the way.</p> <p class="theorem"> The asymptotic rate \(\limsup_{t \to \infty} \sqrt[t]{r_t}\) of momentum is \begin{alignat}{2} &amp;\sqrt{\mom} &amp;&amp;\text{ if }\step \in \big[\frac{(1 - \sqrt{\mom})^2}{\lmin}, \frac{(1+\sqrt{\mom})^2}{L}\big]\\ &amp;\sqrt{\mom}(|\sigma(\lmin)| + \sqrt{\sigma(\lmin)^2 - 1}) &amp;&amp;\text{ if } \step \in \big[0, \min\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 - \sqrt{\mom})^2}{\lmin}\}\big]\\ &amp;\sqrt{\mom}(|\sigma(L)| + \sqrt{\sigma(L)^2 - 1})&amp;&amp;\text{ if } \step \in \big[\max\big\{\tfrac{2(1 + \mom)}{L + \lmin}, \tfrac{(1 + \sqrt{\mom})^2}{L}\big\}, \tfrac{2 (1 + \mom) }{L} \big)\\ &amp;\geq 1 \text{ (divergence)} &amp;&amp; \text{ otherwise.} \end{alignat} </p> <p>Plotting the asymptotic rates for all regions we can see that Polyak momentum (the method with momentum $\mom = \left(\frac{\sqrt{L} - \sqrt{\lmin}}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ and step-size $\step = \left(\frac{2}{\sqrt{L} + \sqrt{\lmin}}\right)^2$ which is asymptotically optimal among the momentum methods with constant coefficients) is at the intersection of the three regions.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum-1400.webp"/> <img src="/staging/assets/img/2022-12-01-hitchhikers-momentum/rate_convergence_momentum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="reproducibility">Reproducibility</h2> <p>All plots in this post were generated using the following Jupyer notebook: <a href="/staging/assets/html/2022-12-01-hitchhikers-momentum/hitchhikers-momentum.html">[HTML]</a> <a href="/staging/assets/html/2022-12-01-hitchhikers-momentum/hitchhikers-momentum.ipynb">[IPYNB]</a></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Polyak momentum is one of the most iconic methods in optimization. Despite it's simplicity, it features rich dynamics that depend both on the step-size and momentum parameter. In this blog post we identify the different regions of the parameter space and discuss their convergence properties using the theory of Chebyshev polynomials.]]></summary></entry><entry><title type="html">How does the inductive bias influence the generalization capability of neural networks?</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/" rel="alternate" type="text/html" title="How does the inductive bias influence the generalization capability of neural networks?"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/"><![CDATA[<p>Deep neural networks are a commonly used machine learning technique that has proven to be effective for many different use cases. However, their ability to generalize from training data is not well understood. In this blog post, we will explore the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, which aims to shed light on the question of why neural networks are able to generalize, and how inductive biases influence their generalization capabilities.</p> <h2 id="overfitting-puzzle">Overfitting Puzzle</h2> <p>One open question in the field of machine learning is the <strong>overfitting puzzle</strong>, which describes the paradox that neural networks are often used in an overparameterized state (i.e., with more parameters than training examples), yet they are still able to generalize well to new, unseen data. This contradicts classical learning theory, which states that a model with too many parameters will simply memorize the training data and perform poorly on new data.</p> <p>Neural networks, particularly deep networks, are typically used in the overparameterized regime, where the number of parameters exceeds the number of training examples. In this case, common generalization bounds do not apply <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. According to classical learning theory, the generalization behavior of a learning system should depend on the number of training examples (n), and the complexity of the model should be balanced with its fit to the data <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. However, neural networks have shown that this is not always the case, as they can perform well even in cases of extreme overparameterization (e.g., a 5-layer CNN with 80 million parameters <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>).</p> <p>To better understand this phenomenon, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> examined the role of inductive bias in neural networks. Inductive bias, or learning bias, refers to the assumptions a network makes about the nature of the target function and is determined by the network’s architecture. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted experiments with different types of fully connected networks (FCN) and convolutional neural networks (CNN) to investigate which biases are effective for these network architectures.</p> <h2 id="experiments">Experiments</h2> <p>In the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, the authors use <strong>empirical studies</strong> to better understand the <em>overfitting puzzle</em> and how inductive bias affects the behavior of overparameterized neural networks. The authors specifically aim to investigate the role of inductive bias under <strong>different architectural choices</strong> by comparing fully connected and convolutional neural networks.</p> <p>The task used in the study is to learn an identity map through a single data point, which is an artificial setup that demonstrates the most extreme case of overparameterization. The goal of the study is to determine whether a network tends towards memorization (learning a constant function) or generalization (learning the identity function).</p> <p>To enable the <strong>identity task</strong> <d-cite key="DBLP:conf/eccv/HeZRS16"></d-cite> for linear models, the authors ensure that hidden dimensions are not smaller than the input and set the weights to the identity matrix in every layer. For convolutional layers, only the center of the kernel is used and all other values are set to zero, simulating a 1 x 1 convolution which acts as a local identity function. For deeper models that use the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation function, it is necessary to encode and recover negative values, as they are discarded by the ReLU function. This can be achieved by using hidden dimensions that are twice the size of the input and storing negative and positive values separately.</p> <p>The study uses the <strong><a href="https://paperswithcode.com/dataset/mnist">MNIST dataset</a></strong> and tests the networks on various types of data, including a linear combination of two digits, random digits from the MNIST test set, random images from the Fashion MNIST dataset, and algorithmically generated image patterns.</p> <p>So let us look at some of the results:</p> <div class="l-page"> <iframe src="/staging/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_3.html" frameborder="0" scrolling="no" width="100%" height="806px"></iframe> </div> <h3 id="fully-connected-networks-fcn">Fully connected networks (FCN)</h3> <p>The figure above shows that for fully connected networks, the outputs differ depending on the depth of the network and the type of testing data. Shallower networks seem to incorporate random white noise into the output, while deeper networks tend to learn the constant function. The similarity of the test data to the training example also affects the behavior of the model. When the test data is from the MNIST digit sets, all network architectures perform quite well. However, for test data that is more dissimilar to the training data, the output tends to include more random white noise. The authors prove this finding with a <em>theorem</em> for 1-layer FCNs,</p> \[f(x) = \Pi(x) + R \Pi(x),\] <p>which decomposes the test data point $x$ into components that are parallel and perpendicular to the training example. This theorem shows that the output becomes more random as the test data becomes less similar to the training data.</p> <p>This behavior can be confirmed by visualizing the results of the 1-layer FCN:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The inductive bias does not lead to either good generalization or memorization. Instead, the predictions become more random as the test data becomes less similar to the training data.</p> <p>Deeper networks tend to learn the constant function, resulting in a strong inductive bias towards the training output regardless of the specific input. This behavior is similar to that of a deep ReLU network, as shown in the figure comparing deep FCN and deep ReLU networks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conclude that more complex network architectures are more prone to memorization. This finding aligns with statistical learning theory, as a more complex architecture has more parameters and, therefore, more overparameterization.</p> <h3 id="convolutional-neural-networks-cnns">Convolutional neural networks (CNNs)</h3> <p>For convolutional neural networks, the inductive bias was analyzed using the ReLU activation function and testing networks with different depths. The hidden layers of the CNN consist of 5 × 5 convolution filters organized into 128 channels. The networks have two constraints to match the structure of the identity target function.</p> <p>If you choose the button ‘CNN’ in the first figure, it shows the resulting visualizations. It can be seen that shallow networks are able to learn the identity function, while intermediate-depth networks function as edge detectors and deep networks learn the constant function. Whether the model learns the identity or the constant function, both outcomes reflect inductive biases since no specific structure was given by the task.</p> <p>A better understanding of the evolution of the output can be obtained by examining the status of the prediction in the hidden layers of the CNN. Since CNNs, unlike FCNs, preserve the spatial relations between neurons in the intermediate layers, these layers can be visualized. The figure below shows the results for a randomly initialized 20-layer CNN compared to different depths of trained CNNs.”</p> <div class="l-page"> <iframe src="/staging/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/CNNs_intermedLayers.html" frameborder="0" scrolling="no" width="100%" height="900px"></iframe> </div> <p>Random convolution gradually smooths out the input data, and after around eight layers, the shapes are lost. When the networks are trained, the results differ. The 7-layer CNN performs well and ends up with an identity function of the input images, while the results of the 14-layer CNN are more blurry. For the 20-layer trained CNN, it initially behaves similarly to the randomly initialized CNN by wiping out the input data, but it preserves the shapes for a longer period. In the last three layers, it renders the constant function of the training data and outputs 7 for any input.</p> <p>These results align with the findings of Radhakrishnan et al. [2018] <d-cite key="radhakrishnan2019memorization"></d-cite> in ‘Memorization in overparametrized autoencoders’, which used a similar empirical framework on fully-connected autoencoders. They found that deep neural networks learn locally contractive maps around the training examples, leading to learning the constant function.</p> <p>As for FCNs, the experiments show that the similarity of the test data to the training data point increases task success. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted further experiments with different <strong>feature channel numbers and dimensions</strong>. They found that increasing the hidden dimensions/adding channels is much less prone to overfitting than adding depth. This should be considered when designing new models: if the goal is to increase the number of parameters of an existing model (perhaps to improve optimization dynamics or prepare for more training data), it is better to try increasing the hidden dimension before tuning the depth, unless the nature of the data changes.</p> <p>Another factor that influences inductive bias is **model initialization++. For networks with few channels, the difference between random initialization and the converged network is extreme <d-cite key="DBLP:conf/iclr/FrankleC19"></d-cite>. This can be explained as follows: in the regime of random initialization with only a few channels, the initialization does not have enough flexibility to compensate for incorrect choices. As a result, the networks are more likely to converge to non-optimal extrema. Having more channels helps to smooth out this problem, as more parameters can compensate for ‘unlucky’ cases.</p> <h2 id="general-findings">General findings</h2> <p>The figures below show that CNNs have better generalization capability than FCNs. However, it is important to note that the experiments primarily aim to compare different neural networks <strong>within their architecture type</strong>, so a comparison between FCNs and CNNs cannot be considered fair. CNNs have natural advantages due to sparser networks and structural biases, such as local receptive fields and parameter sharing, that are consistent with the identity task. Additionally, CNNs have more parameters, as seen in the underlying figure: a 6-layer FCN contains 3.6M parameters, while a 5-layer CNN (with 5x5 filters of 1024 channels) has 78M parameters. These differences should be taken into account when evaluating the results of the experiments.</p> <div class="l-page" style="width: 704px; margin: auto;"> <iframe src="/staging/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/plot.html" frameborder="0" scrolling="no" width="100%" height="480px"></iframe> </div> <p>To conclude, CNNs generalize better than FCNs, even though they have more parameters. This is consistent with the observed phenomenon that neural networks do not follow the statistical learning theory.</p> <p>The experiments described above lead to the following main findings of the paper:</p> <ul> <li>The number of parameters does not strongly correlate with generalization performance, but the structural bias of the model does.</li> </ul> <p>For example, when equally overparameterized,</p> <ul> <li>training a very deep model without residual connections is prone to memorization, while</li> <li>adding more feature channels/dimensions is much less likely to cause overfitting.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>After reading this blog post, we hope that the concept of the overfitting puzzle is understood and that you appreciate the significance of the study conducted by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>. The artificial setup used in the study is a smart way to approach this topic and allows for an intuitive interpretation of the results. The authors found that CNNs tend to “generalize” by actually learning the concept of identity, while FCNs are prone to memorization. Within these networks, it can be said that the simpler the network architecture is, the better the task results. Another observation is that deep CNNs exhibit extreme memorization. It would have been interesting to analyze the inductive bias for other types of data (e.g., sequence data like speech) and compare whether the stated theorems also hold in those cases.</p> <p>In summary, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted interesting studies that have helped the machine learning community to gain a deeper understanding of inductive bias. Their results provide concrete guidance for practitioners that can help design models for new tasks.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[The blog post discusses how memorization and generalization are affected by extreme overparameterization.]]></summary></entry><entry><title type="html">How much meta-learning is in image-to-image translation?</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/" rel="alternate" type="text/html" title="How much meta-learning is in image-to-image translation?"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/how-much-meta-learning-is-in-image-to-image-translation</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/how-much-meta-learning-is-in-image-to-image-translation/"><![CDATA[<p>At the last ICLR conference, Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> presented work showing that CNNs do not transfer information between classes of a classification task.</p> <ul> <li>Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn [ICLR, 2022] Do Deep Networks Transfer Invariances Across Classes?<d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite></li> </ul> <p>Here is a quick summary of their findings: If we train a Convolutional Neural Net (CNN) to classify fruit on a set of randomly brightened and darkened images of apples and oranges, it will learn to ignore the scene’s brightness. We say that the CNN learned that classification is <strong>invariant</strong> to the <strong>nuisance transformation</strong> of randomly changing the brightness of an image. We now add a set of plums to the training data, but fewer examples of them than we have apples and oranges. However, we keep using the same random transformations. The training set thus becomes <strong>class-imbalanced</strong>.</p> <p>We might expect a sophisticated learner to look at the entire dataset, recognize the random brightness modifications across all types of fruit and henceforth ignore brightness when making predictions. If this applied to our fruit experiment, the CNN would be similarly good at ignoring lighting variations on all types of fruit. Furthermore, we would expect the CNN to become more competent at ignoring lighting variations in proportion to <strong>the total amount of images</strong>, irrespective of which fruit they depict.</p> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> show that a CNN does not behave like this: When using a CNN on a <strong>class-imbalanced</strong> classification task with random nuisance transformations, the CNNs invariance to the transformation is proportional to the size of the training set <strong>for each class</strong>. This finding suggests CNNs don’t <strong>transfer invariance</strong> between classes when learning such a classification task.</p> <p>However, there is a solution: Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use an Image to Image translation architecture called MUNIT<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> to learn the transformations and generate additional data from which the CNN can learn the invariance separately for each class. Thus, the invariance to nuisance transformations is transferred <strong>generatively</strong>. They call this method <strong>Generative Invariance Transfer (GIT)</strong>.</p> <p>In this blog post, we are going to argue that:</p> <ul> <li>The experiment described above is a meta-learning experiment.</li> <li>MUNIT is related to meta-learning methods.</li> </ul> <p>Before we proceed to the main post, let’s clarify some definitions. If you are already familiar with the subject, you may skip this part:</p> <details> <summary><b> Definition: Class-Imbalanced Classification</b></summary> <br/> <p> In many real-world classification datasets, the number of examples for each class varies. Class-imbalanced classification refers to classification on datasets where the frequencies of class labels vary significantly. </p> <p> It is generally more difficult for a neural network to learn to classify classes with fewer examples. However, it is often important to perform well on all classes, regardless of their frequency in the dataset. If we train a model to classify a dataset of different skin tumors, most examples may be benign. Still, it is crucial to identify the rare, malignant ones. Experiment design, including training and evaluation methods must therefore be adjusted when using class-imbalanced data. </p> <br/> </details> <details> <summary><b> Definition: Nuisance Transformation &amp; Transformation Invariance</b></summary> <br/> <p> Transformations are alterations of data. In the context of image classification, nuisance transformations are alterations that do not affect the class labels of the data. A model is said to be invariant to a nuisance transformation if it can successfully ignore the transformation when predicting a class label. </p> We can formally define a nuisance transformation <p> $$T(\cdot |x)$$ </p> <p> as a distribution over transformation functions. An example of a nuisance transformation might be a distribution over rotation matrices of different angles, or lighting transformations with different exposure values. By definition, nuisance transformations have no impact on class labels $y$, only on data $x$. A perfectly transformation-invariant classifier would thus completely ignore them, i.e., </p> <p> $$ \hat{P}_w(y = j|x) = \hat{P}_w(y = j|x'), \; x' \sim T(\cdot |x). $$ </p> </details> <h2 id="a-closer-look-at-the-experiment">A closer look at the experiment</h2> <p>Let’s take a more detailed look at the experiment Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> conducted:</p> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> take a dataset, e.g., CIFAR-100, then apply a nuisance transformation, for example, random rotation, background intensity, or dilation and erosion. They then remove samples from some classes until the distribution of class sizes follows Zipf’s law with parameter 2.0 and a minimum class size of 5. The test set remains balanced, i.e., all test classes have the same number of samples. They then train a CNN model - for example, a ResNet - on this imbalanced and transformed training data.</p> <p>To measure the invariance of the trained model to the applied transformation Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use the empirical Kullback-Leibler divergence between the untransformed test set and the transformed test set of each class.</p> <p> $$ eKLD(\hat{P}_w) = \mathbb{E}_{x \sim \mathbb{P}_{test}, x' \sim T(\cdot|x)} [D_{KL}(\hat{P}_w(y = j|x) || \hat{P}_w(y = j|x'))] $$ </p> <p>If the learner is invariant to the transformation, the predicted probability distribution over class labels should be identical for the transformed and untransformed images. In that case, the KLD should be zero and greater than zero otherwise. The higher the expected KL-divergence, the more the applied transformation impacts the network’s predictions.</p> <p>The result: eKLD falls with class size. This implies that the CNN does not learn that there are the same nuisance transformations on all images and therefore does not transfer this knowledge to the classes with less training data. A CNN learns invariance <strong>separately for each class</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/EKLD.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="how-is-this-a-meta-learning-experiment">How is this a meta-learning experiment?</h2> <p>You might think this is a cool experiment, but how is it related to meta-learning?</p> <p>Let’s look at one of the original papers on meta-learning. In the 1998 book “Learning to learn” Sebastian Thrun &amp; Lorien Pratt define an algorithm as capable of “Learning to learn” if it improves its performance in proportion to the number of tasks it is exposed to:</p> <blockquote> <p>an algorithm is said to learn to learn if its performance at each task improves with experience and with the number of tasks. Put differently, a learning algorithm whose performance does not depend on the number of learning tasks, which hence would not benefit from the presence of other learning tasks, is not said to learn to learn <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite></p> </blockquote> <p>Now how does this apply to the experiment just outlined? In the introduction, we thought about how a sophisticated learner might handle a dataset like the one described in the last section. We said that a sophisticated learner would learn that the nuisance transformations are applied uniformly <strong>to all classes</strong>. Therefore, if we added more classes to the dataset, the learner would become <strong>more invariant</strong> to the transformations because we expose it to more examples of them. Since this is part of the classification task <strong>for each class</strong>, the learner should, everything else being equal, become better at classification, especially on classes with few training examples. To see this, we must think of the multi-classification task not as a single task but as multiple mappings from image features to activations that must be learned, as a set of binary classification tasks. Thrun and Pratt continue:</p> <blockquote> <p>For an algorithm to fit this definition, some kind of <em>transfer</em> must occur between multiple tasks that must have a positive impact on expected task-performance <d-cite key="DBLP:books/sp/98/ThrunP98"></d-cite>.</p> </blockquote> <p>This transfer is what Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> tried to measure. There is some meta-information learnable across several tasks, in our case, the transformation distribution across many binary classification tasks. If a learner can learn this meta-information and transfer it to each new task it has “learned to learn”; it is a meta-learner. The goal of Zhou et al.’s <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> experiment was to see whether this transfer takes place. Thus, arguably, it is a meta-learning experiment.</p> <h2 id="generative-invariance-transfer">Generative Invariance Transfer</h2> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> don’t stop there. They show that, using the MUNIT (Multimodal Unsupervised image-to-image Translation)<d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> architecture, they can learn the nuisance transformations applied to the dataset and generate additional training samples for the classes with few samples, improving transformation invariance there. They call this Generative invariance transfer (GIT). Let’s take a closer look:</p> <p>MUNIT networks are capable of performing image-to-image translation, which means that they can translate an image from one domain, such as pictures of leopards, into another domain, such as pictures of house cats. The translated image should look like a real house cat while still resembling the original leopard image. For instance, if the leopard in the original image has its eyes closed, the translated image should contain a house cat with closed eyes. Eye state is a feature present in both domains, so a good translator should not alter it. On the other hand, a leopard’s fur is yellow and spotted, while a house cat’s fur can be white, black, grey, or brown. To make the translated images indistinguishable from real house cats, the translator must thus replace leopard fur with house cat fur.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_ENCODING.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT networks learn to perform translations by correctly distinguishing the domain-agnostic features (such as eye state) from the domain-specific features (such as the distribution of fur color). They embed an image into two latent spaces: a content space that encodes the domain-agnostic features and a style space that encodes the domain-specific features (see figure above).</p> <p>To transform a leopard into a house cat, we can encode the leopard into a content and a style code, discard the leopard-specific style code, randomly select a cat-specific style code, and assemble a house cat image that looks similar by combining the leopard’s content code with the randomly chosen cat style code (see figure below).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_TRANSLATION.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> modify the process of using MUNIT to transfer images between domains. They do not use MUNIT to translate images <strong>between</strong> domains but <strong>within</strong> a domain. The MUNIT network exchanges the style code of an image with another style code of the same domain. For example, if the domain is house cats, the MUNIT network might translate a grey house cat into a black one. The learning task in this single-domain application of MUNIT is to decompose example-agnostic content features from example-specific style features so that the translated images still look like house cats. For example, fur color is a valid style feature for translating within the ‘house cat’ domain because every house cat has a fur color. A translator only switching fur color is hard to detect.</p> <p>However, if the domain included house cats <strong>and apples</strong>, fur color is not a valid style feature. If it was, the translator might translate fur color on an apple and give it black fur, which would look suspiciously out of place. Whatever house cats and apples have in common - maybe their position or size in the frame - would be a valid style feature. We would expect an intra-domain translator on an apples-and-cats dataset to change the position and size of an apple but not to turn it into a cat (not even partially).</p> <p>It turns out that on a dataset with uniformly applied nuisance transformations, the nuisance transformations are valid style features: The result of randomly rotating an apple cannot be discerned as artificial when images of all classes, house cats and apples, were previously randomly rotated.</p> <p>Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> find that when they train a MUNIT network on a dataset with nuisance transformations and class imbalances, the MUNIT network decomposes the class and transformation distributions. The style latent space of the MUNIT network approximates the transformation distribution $T(\cdot |x)$. The content space preserves the remaining features of the image, such as its class. Thus, when translating an image, i.e., exchanging its style code, MUNIT applies a random nuisance transformation while preserving content. Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> use this method to generate data for classes with few examples. While the CNN is still unable to transfer invariance to $T(\cdot |x)$ between classes, it can now learn it for each class separately using the data generated by MUNIT, which has acquired knowledge of $T(\cdot |x)$ from the entire dataset.</p> <p>So MUNIT can decompose the example-specific information, e.g., whether something is an apple or a house cat, from the meta-information, the applied nuisance transformations. When we add more classes, it has more data and can better learn the transformation distribution T(\cdot |x)$. Does solving a meta-learning problem make MUNIT a meta-learner? Let’s look at the relationship MUNIT has with traditional meta-learners.</p> <h2 id="how-much-meta-learning-is-in-munit">How much meta-learning is in MUNIT?</h2> <p>To see how well MUNIT fits the definition of meta-learning, let’s define meta-learning more concretely. We define contemporary neural-network-based meta-learners in terms of a learning procedure: An outer training loop with a set of trainable parameters iterates over tasks in a distribution of tasks. Formally a task is comprised of a dataset and a loss function $ \mathcal{T} = \{ \mathcal{D}, \mathcal{L} \} $. In an inner loop, a learning algorithm based on the outer loop’s parameters is instantiated for each task. We train it on a training set (<em>meta-training</em>) and test it on a validation set (<em>meta-validation</em>). We then use loss on this validation set to update the outer loop’s parameters. In this task-centered view of meta-learning, we can express the objective function as</p> <p> $$ \underset{\omega}{\mathrm{min}} \; \mathbb{E}_{\mathcal{T} \sim p(\mathcal{T})} \; \mathcal{L}(\mathcal{D}, \omega), $$ </p> <p>where $ \omega $ is parameters trained exclusively on the meta-level, i.e., the <em>meta-knowledge</em> learnable from the task distribution <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>This <em>meta-knowledge</em> is what the meta-learner accumulates and transfers across the tasks in Thrun and Pratt’s definition above. Collecting meta-knowledge allows the meta-learner to improve its expected task performance with the number of tasks. The meta-knowledge in the experiment of Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> is the invariance to the nuisance transformations as the transformations are identical and need to be ignored for images of all classes. By creating additional transformed samples, the MUNIT network makes the meta-knowledge learnable for the CNN.</p> <p>The task-centered view of meta-learning brings us to a related issue: A meta-learner must discern and decompose task-specific knowledge from meta-knowledge. Contemporary meta-learners decompose meta-knowledge through the different objectives of their inner and outer loops and their respective loss terms. They store meta-knowledge in the outer loop’s parameter set $ \omega $ but must not learn task-specific information there. Any unlearned meta-features lead to slower adaptation, negatively impacting performance, <em>meta-underfitting</em>. On the other hand, any learned task-specific features will not generalize to unseen tasks in the distribution, thus also negatively impacting performance, <em>meta-overfitting</em>.</p> <p>We recall that, similarly, MUNIT <d-cite key="DBLP:conf/eccv/HuangLBK18"></d-cite> decomposes domain-specific style information and domain-agnostic content information. Applied to two domains, leopards and house cats, a MUNIT network will encode the domain-agnostic information, e.g., posture, scale, background, in its content latent space, and the domain-specific information, e.g., how a cat’s hair looks, in its style latent space. If the MUNIT network encoded the domain-agnostic information in the style latent space, the resulting image would not appear to be a good translation since the style information is discarded and replaced. It might turn a closed-eyed leopard into a staring cat. If the MUNIT network encoded the domain-specific transformation in the content latent space, the network would have difficulty translating between domains. A house cat might still have its original leopard fur.</p> <p>Both meta-learning and multi-domain unsupervised image-to-image translation are thus learning problems that require a separation of the general from the specific. This is even visible when comparing their formalizations as optimization problems.</p> <p>Francheschi et al. [2018] <d-cite key="DBLP:conf/icml/FranceschiFSGP18"></d-cite> show that all contemporary neural-network-based meta-learning approaches can be expressed as bi-level optimization problems. We can formally write the optimization objective of a general meta-learner as:</p> <p> $$ \bbox[5pt, border: 2px solid blue]{ \begin{align*} \omega^{*} = \underset{\omega}{\mathrm{argmin}} \sum_{i=1}^{M} \mathcal{L}^{meta}(\theta^{* \; (i)}(\omega), D^{val}_i), \end{align*} } $$ </p> <p>where $M$ describes the number of tasks in a batch, $\mathcal{L}^{meta}$ is the meta-loss function, and $ D^{val}_i $ is the validation set of the task $ i $. $\omega$ represents the parameters exclusively updated in the outer loop. $ \theta^{* \; (i)} $ represents an inner loop learning a task that we can formally express as a sub-objective constraining the primary objective</p> <p> $$ \bbox[5pt, border: 2px solid red]{ \begin{align*} s.t. \; \theta^{* \; (i)} = \underset{\theta}{\mathrm{argmin}} \; \mathcal{L^{task}}(\theta, \omega, D^{tr}_i), \end{align*} } $$ </p> <p>where $ \theta $ are the model parameters updated in the inner loop, $ \mathcal{L}^{task} $ is the loss function by which they are updated and $ D^{tr}_i $ is the training set of the task $ i $ <d-cite key="DBLP:journals/pami/HospedalesAMS22"></d-cite>.</p> <p>It turns out that the loss functions of MUNIT can be similarly decomposed:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg-1400.webp"/> <img src="/staging/assets/img/2022-12-01-how-much-meta-learning-is-in-image-to-image-translation/MUNIT_LOSS.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MUNIT’s loss function consists of two adversarial (GAN) <d-cite key="DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite> loss terms (see figure above) with several auxiliary reconstruction loss terms. To keep the notation simple, we combine all reconstruction terms into a joined reconstruction loss $ \mathcal{L}_{recon}(\theta_c, \theta_s) $, where $ \theta_c $ are the parameters of the <em>content</em> encoding/decoding networks and $ \theta_s $ are the parameters of the <em>style</em> encoding/decoding networks. We will only look at one of the two GAN losses in detail since they are symmetric, and one is discarded entirely when MUNIT is used on a single domain in the fashion of Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite>.</p> <p>MUNIT’s GAN loss term is</p> <p> $$ \begin{align*} &amp;\mathcal{L}^{x_{2}}_{GAN}(\theta_d, \theta_c, \theta_s) \\\\ =&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \;\mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], \end{align*} $$ </p> <p>where the $ \theta_d $ represents the parameters of the discriminator network, $p(x_2)$ is the data of the second domain, $ c_1 $ is the content embedding of an image from the first domain to be translated. $ s_2 $ is a random style code of the second domain. $ D_2 $ is the discriminator of the second domain, and $ G_2 $ is its generator. MUNIT’s full objective function is:</p> <p> $$ \begin{align*} \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \underset{\theta_d}{\mathrm{argmax}}&amp; \;\mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ +&amp; \; \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right], + \; \mathcal{L}^{x_{1}}_{GAN}(\theta_d, \theta_c, \theta_s) \\ +&amp; \;\mathcal{L}_{recon}(\theta_c, \theta_s) \end{align*} $$ </p> <p>(compare <d-cite key="DBLP:conf/eccv/HuangLBK18, DBLP:conf/nips/GoodfellowPMXWOCB14"></d-cite>). We can reformulate this into a bi-level optimization problem by extracting a minimization problem describing the update of the generative networks. We also drop the second GAN loss term as it is not relevant to our analysis.</p> <p> $$ \bbox[5px, border: 2px solid blue]{ \begin{align*} \omega^{*} &amp; = \{ \theta_c^*, \theta_s^* \} \\\\ &amp; = \underset{\theta_c, \theta_s}{\mathrm{argmin}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d^{*})) \right] \\ &amp; + \mathcal{L}_{recon}(\theta_c, \theta_s), \end{align*} } $$ </p> <p>We then add a single constraint, a subsidiary maximization problem for the discriminator function:</p> <p> $$ \bbox[5px, border: 2px solid red]{ \begin{align*} &amp;s.t. \;\theta_d^{*} \\\\ &amp; = \underset{\theta_d}{\mathrm{argmax}} \; \mathbb{E}_{c_{1} \sim p(c_{1}), s_{2} \sim p(s_{2})} \left[ \log (1 -D_ {2} (G_{2} (c_{1}, s_{2}, \theta_c, \theta_s), \theta_d)) \right] \\ &amp; + \mathbb{E}_{x_{2} \sim p(x_{2})} \left[ \log(D_{2} (x_{2}, \theta_d)) \right] \end{align*} } $$ </p> <p>Interestingly, this bi-level view does not only resemble a meta-learning procedure as expressed above, but the bi-level optimization also facilitates a similar effect. Maximizing the discriminator’s performance in the constraint punishes style information encoded as content information. If style information is encoded as content information, the discriminator detects artifacts of the original domain in the translated image. Similarly, a meta-learner prevents <em>meta-overfitting</em> via an outer optimization loop.</p> <p>The two procedures also share “indirect” parameter updates. During GAN training, the discriminator’s parameters are updated through the changes in the generator’s parameters, which derive from the discriminator’s previous parameters, and so forth; The training of the discriminator and generator are separate but dependent processes. Similarly, in a meta-learner, the outer loop impacts the inner loop by determining its initial learner. The results of the inner loop, meanwhile, impact the outer loop via the meta-validation loss. While often overlapping in practice, the inner and outer loop parameter sets could, in principle, be completely disjunct, as they are in GAN training.</p> <p>Concluding, we discern between MUNIT applied to two domains versus a single domain. When applied to two domains, MUNIT is a <em>binary</em> image-to-image translation architecture, i.e., we cannot add domains. For multi-domain translation, we need to train many MUNIT networks to translate between pairs of domains. Thus, although it uses mechanisms similar to a meta-learner, it is <em>not</em> a meta-learner in an image-to-image translation context.</p> <p>However, when applied to a single domain MUNIT <em>does</em> “learn to learn” (if you agree with the conclusion of the previous chapter) as it combines information from all classes to extract the transformation distribution. While it does not <em>perform</em> classification, the class information of an image is encoded in MUNIT’s content space. Since MUNIT is trained unsupervised, it is probably closer to a distance metric than an actual class label. We might thus classify single-domain MUNIT as an unsupervised, generative meta-learner. It performs meta-learning in a general sense of “discerning the general from the task-specific” using a related two-loop training approach with two sets of parameters and a similar loss function. Thus MUNIT is related to meta-learning methods.</p> <h2 id="conclusion">Conclusion</h2> <p>The ICLR paper “Do Deep Networks Transfer Invariances Across Classes?” <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> shows that image-to-image translation methods can be used to learn and apply nuisance transformations, enabling a CNN to become invariant to them via data augmentation. This blog post argued that this is a meta-learning setting. In the view of this author, Zhou et al. [2022] <d-cite key="DBLP:conf/iclr/ZhouTRKPHF22"></d-cite> solve a meta-learning problem using an unsupervised, generative method. A closer examination reveals parallels between both types of architecture.</p> <p><em>Learning the meta-information of image-to-image translation and meta-learning might enable researchers to design better architectures in both domains.</em></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[...in which we find a connection between meta-learning literature and a paper studying how well CNNs deal with nuisance transforms in a class-imbalanced setting. Closer inspection reveals a surprising amount of similarity - from meta-information to loss functions.]]></summary></entry><entry><title type="html">Thinking Like Transformers</title><link href="https://iclr-blogposts.github.io/staging/blog/2022/raspy/" rel="alternate" type="text/html" title="Thinking Like Transformers"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/staging/blog/2022/raspy</id><content type="html" xml:base="https://iclr-blogposts.github.io/staging/blog/2022/raspy/"><![CDATA[<link rel="stylesheet" href="custom.css"/> <h1 id="thinking-like-transformers">Thinking Like Transformers</h1> <ul> <li><a href="https://arxiv.org/pdf/2106.06981.pdf">Paper</a> by Gail Weiss, Yoav Goldberg, Eran Yahav</li> </ul> <p>Transformer models are foundational to AI systems. There are now countless explanations of “how transformers work?” in the sense of the architecture diagram at the heart of transformers.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_5_0.svg" alt="svg"/></p> <p>However this diagram does not provide any intuition into the computational model of this framework. As researchers become interested in how Transformers work, gaining intuition into their mechanisms becomes increasingly useful.</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf">Thinking like Transformers</a> proposes a computational framework for Transformer-like calculations. The framework uses discrete computation to simulate Transformer computations. The resulting language <a href="https://github.com/tech-srl/RASP">RASP</a> is a programming language where, ideally, every program can compile down to a specific Transformer (indeed, David Lindner and colleagues have recently released a <a href="https://arxiv.org/abs/2301.05062">compiler</a> for a large subset of RASP!).&lt;/p&gt;</p> <p>In this blog post, I reimplemented a variant of RASP in Python (RASPy). The language is roughly compatible with the original version, but with some syntactic changes that I thought were fun. With this language, we have a challenging set of puzzles to walk through and understand how it works.</p> <p>Before jumping into the language itself, let’s look at an example of what coding with Transformers looks like. Here is some code that computes the <code class="language-plaintext highlighter-rouge">flip</code>, i.e. reversing an input sequence. The code itself uses two Transformer layers to apply attention and mathematical computations to achieve the result.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">flip</span><span class="p">():</span>
    <span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">flip</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flip</span>
<span class="n">flip</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_11_0.svg" alt="svg"/></p> <h2 id="table-of-contents">Table of Contents</h2> <ul> <li>Part 1: <a href="#transformers-as-code">Transformers as Code</a></li> <li>Part 2: <a href="#coding-with-transformers">Coding with Transformers</a></li> </ul> <h2 id="transformers-as-code">Transformers as Code</h2> <p>Our goal is to define a computational formalism that mimics the expressivity of Transformers. We will go through this process by analogy, describing each language construct next to the aspect of the Transformer it represents. (See the full <a href="https://arxiv.org/pdf/2106.06981.pdf">paper</a> for the formal language specification).</p> <p>The core unit of the language is a <em>sequence operation</em> that transforms a sequence to another sequence of the same length. I will refer to these throughout as <em>transforms</em>.</p> <h3 id="inputs">Inputs</h3> <p>In a Transformer, the base layer is the input fed to the model. This input usually contains the raw tokens as well as positional information.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_15_0.svg" alt="svg"/></p> <p>In code, the symbol <code class="language-plaintext highlighter-rouge">tokens</code> represents the simplest transform. It returns the tokens passed to the model. The default input is the sequence “hello”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_17_0.svg" alt="svg"/></p> <p>If we want to change the input to the transform, we use the input method to pass in an alternative.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span><span class="p">.</span><span class="nb">input</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_19_0.svg" alt="svg"/></p> <p>As with Transformers, we cannot access the positions of these sequences directly. However, to mimic position embeddings, we have access to a sequence of indices.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">indices</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_21_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sop</span> <span class="o">=</span> <span class="n">indices</span>
<span class="n">sop</span><span class="p">.</span><span class="nb">input</span><span class="p">(</span><span class="s">"goodbye"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_22_0.svg" alt="svg"/></p> <h3 id="feed-forward-network">Feed Forward Network</h3> <p>After the input layer, we reach the feed-forward network. In a Transformer, this stage can apply mathematical operations to each element of the sequence independently.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_24_0.svg" alt="svg"/></p> <p>In code, we represent this stage by computation on transforms. Mathematical operations are overloaded to represent independent computation on each element of the sequence .</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">==</span> <span class="s">"l"</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_26_0.svg" alt="svg"/></p> <p>The result is a new transform. Once constructed it can be applied to new input.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">*</span> <span class="mi">2</span>  <span class="o">-</span> <span class="mi">1</span>
<span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_28_0.svg" alt="svg"/></p> <p>Operations can combine multiple transforms. For example, functions of <code class="language-plaintext highlighter-rouge">tokens</code> and <code class="language-plaintext highlighter-rouge">indices</code>. The analogy here is that the Transformer activations can keep track of multiple pieces of information simultaneously.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">-</span> <span class="mi">5</span> <span class="o">+</span> <span class="n">indices</span>
<span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_30_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="s">"l"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">indices</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_31_0.svg" alt="svg"/></p> <p>We provide a few helper functions to make it easier to write transforms. For example, <code class="language-plaintext highlighter-rouge">where</code> provides an “if” statement like construct</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">where</span><span class="p">((</span><span class="n">tokens</span> <span class="o">==</span> <span class="s">"h"</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="s">"l"</span><span class="p">),</span> <span class="n">tokens</span><span class="p">,</span> <span class="s">"q"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_33_0.svg" alt="svg"/></p> <p>And <code class="language-plaintext highlighter-rouge">map</code> lets us define our own operators, for instance a string to int transform. (Users should be careful to only use operations here that could be computed with a simple neural network).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">atoi</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nb">ord</span><span class="p">(</span><span class="s">'0'</span><span class="p">))</span>
<span class="n">atoi</span><span class="p">.</span><span class="nb">input</span><span class="p">(</span><span class="s">"31234"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_35_0.svg" alt="svg"/></p> <p>When chaining these transforms, it is often easier to work with functions. For example the following applies where and then <code>atoi</code> and then adds 2.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">atoi</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="nb">ord</span><span class="p">(</span><span class="s">'0'</span><span class="p">))</span> 

<span class="n">op</span> <span class="o">=</span> <span class="p">(</span><span class="n">atoi</span><span class="p">(</span><span class="n">where</span><span class="p">(</span><span class="n">tokens</span> <span class="o">==</span> <span class="s">"-"</span><span class="p">,</span> <span class="s">"0"</span><span class="p">,</span> <span class="n">tokens</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">op</span><span class="p">.</span><span class="nb">input</span><span class="p">(</span><span class="s">"02-13"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_37_0.svg" alt="svg"/></p> <p>From here on, unless we use a different input sequence, we will assume that the input is ‘hello’ and omit the input display in the illustrations.</p> <h3 id="attention-selectors">Attention Selectors</h3> <p>Things get more interesting when we start to apply attention. This allows routing of information between the different elements of the sequence.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_39_0.svg" alt="svg"/></p> <p>We begin by defining notation for the keys and queries of the model. Keys and queries are effectively transforms that we will broadcast and compare to each other to create <em>selectors</em>, our parallel to attention patterns. We create them directly from transforms. For example, if we want to define a key, we call <code class="language-plaintext highlighter-rouge">key</code> on a transform.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_41_0.svg" alt="svg"/></p> <p>Similarly for <code class="language-plaintext highlighter-rouge">query</code>. (Queries are presented as columns to reflect their relation to the selectors we will create from them.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_43_0.svg" alt="svg"/></p> <p>Scalars can be used as keys or queries. They broadcast out to the length of the underlying sequence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_45_0.svg" alt="svg"/></p> <p>By applying a comparison operation between a key and a query we create a <em>selector</em>, our parallel to an attention matrix - though this one is unweighted.</p> <p>A selector is a binary matrix indicating which input position (column) each output position (row) will attend to in an eventual attention computation. In the comparison creating it, the key values describe the input (column) positions, and the query values describe the output (row) positions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eq</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">eq</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_47_0.svg" alt="svg"/></p> <p>Some examples:</p> <ul> <li>A selector that matches each output position to the previous input position.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">offset</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_49_0.svg" alt="svg"/></p> <ul> <li>A selector that matches each output position to all earlier input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">=</span> <span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">before</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_51_0.svg" alt="svg"/></p> <ul> <li>A selector that matches each output position to all later input positions.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">after</span> <span class="o">=</span> <span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">after</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_53_0.svg" alt="svg"/></p> <p>Selectors can be merged using boolean operations. For example, this selector focuses each output position on 1) earlier positions that 2) contain the same original input token as its own. We show this by including both pairs of keys and queries in the matrix.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">before</span> <span class="o">&amp;</span> <span class="n">eq</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_55_0.svg" alt="svg"/></p> <h2 id="using-attention">Using Attention</h2> <p>Given an attention selector we can provide a value sequence to aggregate. We represent aggregation by <strong>summing</strong> up over the values that have a true value for their selector.</p> <p>(Note: in the original paper, they use a <strong>mean</strong> aggregation and show a clever construction where mean aggregation is able to represent a sum calculation. RASPy uses sum by default for simplicity and to avoid fractions. In practicce this means that RASPy may underestimate the number of layers needed to convert to a mean based model by a factor of 2.)</p> <p>Attention aggregation gives us the ability to compute functions like histograms.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">tokens</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_59_0.svg" alt="svg"/></p> <p>Visually we follow the architecture diagram. Queries are to the left, Keys at the top, Values at the bottom, and the Output is to the right.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_61_0.svg" alt="svg"/></p> <p>Some attention operations may not even use the input tokens. For instance to compute the <code class="language-plaintext highlighter-rouge">length</code> of a sequence, we create a “select all” attention selector and then add 1 from each position.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">length</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"length"</span><span class="p">)</span>
<span class="n">length</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_63_0.svg" alt="svg"/></p> <p>Here’s a more complex example, shown step-by-step. (This is the kind of thing they ask in interviews!)</p> <p>Say we want to compute the sum of neighboring values in a sequence, along a sliding window. First we apply the forward cutoff, attending only to positions that are not too far in the past.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">WINDOW</span><span class="o">=</span><span class="mi">3</span>
<span class="n">s1</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span> <span class="o">-</span> <span class="n">WINDOW</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>  
<span class="n">s1</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_65_0.svg" alt="svg"/></p> <p>Then the backward cutoff, attending only to positions up to and including our own.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s2</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>
<span class="n">s2</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_67_0.svg" alt="svg"/></p> <p>Intersect.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sel</span> <span class="o">=</span> <span class="n">s1</span> <span class="o">&amp;</span> <span class="n">s2</span>
<span class="n">sel</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_69_0.svg" alt="svg"/></p> <p>And finally aggregate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sum2</span> <span class="o">=</span> <span class="n">sel</span><span class="p">.</span><span class="n">value</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> 
<span class="n">sum2</span><span class="p">.</span><span class="nb">input</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_71_0.svg" alt="svg"/></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum. The cumulative sum has to go into a second layer because it is applied to a transform which uses length, and so it can only be computed after the computation of length is complete.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">before</span> <span class="o">|</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">))).</span><span class="n">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"cumsum"</span><span class="p">)</span>
<span class="n">cumsum</span><span class="p">().</span><span class="nb">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_73_0.svg" alt="svg"/></p> <h2 id="layers">Layers</h2> <p>The language supports building up more complex transforms. It keeps track of the <em>layers</em> by tracking the operations computed so far.</p> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_76_0.svg" alt="svg"/></p> <p>Here is a simple example that produces a 2-layer transform. The first corresponds to computing length and the second the cumulative sum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="n">indices</span><span class="p">)</span>
<span class="n">x</span><span class="p">.</span><span class="nb">input</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_78_0.svg" alt="svg"/></p> <h2 id="coding-with-transformers">Coding with Transformers</h2> <p>Given this library of functions, we can write operations to accomplish surprisingly complex tasks.</p> <p><strong>Can we produce a Transformer that does basic addition of two arbitrary length numbers?</strong></p> <p>i.e. given a string “19492+23919” can we produce the correct output?</p> <p>We will go through these steps, and their solutions, here. If you would rather do them on your own, we provide a version where you can try them yourself!</p> <p>Before we dive in to the main task, we will do some challenges of increasing difficulty to help us build some intuitions.</p> <h3 id="challenge-1-select-a-given-index">Challenge 1: Select a given index</h3> <p>Produce a sequence where all the elements have the value at index i.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">i</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"index"</span><span class="p">)</span>
<span class="n">index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_83_0.svg" alt="svg"/></p> <h3 id="challenge-2-shift">Challenge 2: Shift</h3> <p>Shift all of the tokens in a sequence to the right by i positions. (Here we introduce an optional parameter in the aggregation: the default value to be used when no input positions are selected. If not defined, this value is 0.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"_"</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="o">-</span><span class="n">i</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"shift"</span><span class="p">)</span>
<span class="n">shift</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_85_0.svg" alt="svg"/></p> <h3 id="challenge-3-minimum">Challenge 3: Minimum</h3> <p>Compute the minimum values of the sequence. (This one starts to get harder. Our version uses 2 layers of attention.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">sel1</span> <span class="o">=</span> <span class="n">before</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>
    <span class="n">sel2</span> <span class="o">=</span> <span class="n">key</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">query</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">less</span> <span class="o">=</span> <span class="p">(</span><span class="n">sel1</span> <span class="o">|</span> <span class="n">sel2</span><span class="p">).</span><span class="n">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">less</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"min"</span><span class="p">)</span>
<span class="n">minimum</span><span class="p">()([</span><span class="mi">5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_87_0.svg" alt="svg"/></p> <p>The idea behind our solution is an implicit full ordering of the input positions: we (implicitly) order the positions according to input token value, with input position as tie breaker. Our first act is to have each position attend to all positions before it in the ordering: <code class="language-plaintext highlighter-rouge">sel1</code> focuses on earlier input positions with the same input token value, and <code class="language-plaintext highlighter-rouge">sel2</code> focuses on input positions with lower input token value. We then aggregate a 1 from all positions to get where each position is located in this ordering (i.e., how many other positions precede it). The minimum value is the input value at the first position according to this ordering (i.e., the one which had no other positions precede it).</p> <h3 id="challenge-4-first-index">Challenge 4: First Index</h3> <p>Compute the first index that has token q, assuming the sequence always has length shorter than 100. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">first</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">minimum</span><span class="p">(</span><span class="n">where</span><span class="p">(</span><span class="n">seq</span> <span class="o">==</span> <span class="n">q</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="mi">99</span><span class="p">))</span>
<span class="n">first</span><span class="p">(</span><span class="s">"l"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_90_0.svg" alt="svg"/></p> <h3 id="challenge-5-right-align">Challenge 5: Right Align</h3> <p>Right align a padded sequence e.g. ralign().inputs(‘xyz___’) = ‘—xyz’” (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ralign</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s">"-"</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="s">"_"</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">indices</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">indices</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"ralign"</span><span class="p">)</span>
<span class="n">ralign</span><span class="p">()(</span><span class="s">"xyz__"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_92_0.svg" alt="svg"/></p> <h3 id="challenge-6-split">Challenge 6: Split</h3> <p>Split a sequence into two parts at value v and then right align. You can assume there is exactly one appearance of v in the sequence. (3 layers to get and align the first part of the sequence, but only 1 for the second.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">get_first_part</span><span class="p">,</span> <span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"0"</span><span class="p">):</span>
    <span class="n">split_point</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">sop</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">v</span><span class="p">)).</span><span class="n">value</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">get_first_part</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ralign</span><span class="p">(</span><span class="n">default</span><span class="p">,</span> 
                   <span class="n">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&lt;</span> <span class="n">split_point</span><span class="p">,</span> 
                         <span class="n">sop</span><span class="p">,</span> <span class="s">"_"</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">indices</span> <span class="o">&gt;</span> <span class="n">split_point</span><span class="p">,</span> <span class="n">sop</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="n">split</span><span class="p">(</span><span class="s">"+"</span><span class="p">,</span> <span class="bp">False</span><span class="p">)(</span><span class="s">"xyz+zyr"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_94_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">split</span><span class="p">(</span><span class="s">"+"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="s">"xyz+zyr"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_95_0.svg" alt="svg"/></p> <h3 id="challenge-6-slide">Challenge 6: Slide</h3> <p>Replace special tokens “&lt;” with the closest non “&lt;” value to their right. (2 layers)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">slide</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> 
    <span class="n">y</span> <span class="o">=</span> <span class="p">((</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">key</span><span class="p">(</span><span class="n">match</span><span class="p">)</span> <span class="o">==</span> <span class="n">query</span><span class="p">(</span><span class="bp">True</span><span class="p">))).</span><span class="n">value</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span>  <span class="n">where</span><span class="p">(</span><span class="n">match</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">seq</span><span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">"slide"</span><span class="p">)</span>
<span class="n">slide</span><span class="p">(</span><span class="n">tokens</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">).</span><span class="nb">input</span><span class="p">(</span><span class="s">"xxxh&lt;&lt;&lt;l"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_97_0.svg" alt="svg"/></p> <h3 id="challenge-7-add">Challenge 7: Add</h3> <p>For this one you want to perform addition of two numbers. Here are the steps.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">add</span><span class="p">().</span><span class="nb">input</span><span class="p">(</span><span class="s">"683+345"</span><span class="p">)</span>
</code></pre></div></div> <ol> <li>Split into parts (challenge 6). Convert to ints. Add.</li> </ol> <blockquote> <p>“683+345” =&gt; [0, 0, 0, 9, 12, 8]</p> </blockquote> <ol> <li>Compute the carry terms. Three possibilities: definitely receives carry (“1”), definitely doesn’t receive carry (“0”), maybe receives carry (“&lt;”).Because we are only adding two numbers, the only case in which a position might receive a carry is if the position after it sums to 9. In that case, it will receive a carry if and only if the position after <em>that</em> receives a carry.</li> </ol> <blockquote> <p>[0, 0, 0, 9, 12, 8] =&gt; “00&lt;100”</p> </blockquote> <ol> <li>Slide the carry coefficients. A position that might receive a carry will get one if and only if the next position receives a carry - and so on down the chain until the next definite carry/no carry.</li> </ol> <blockquote> <p>“00&lt;100” =&gt; 001100”</p> </blockquote> <ol> <li>Complete the addition.</li> </ol> <p>Each of these is 1 line of code. The full system is 6 layers. (if you are careful you can do it in 5!).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">sop</span><span class="o">=</span><span class="n">tokens</span><span class="p">):</span>
    <span class="c1"># 0) Parse and add
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="s">"+"</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span> \
        <span class="o">+</span> <span class="n">atoi</span><span class="p">(</span><span class="n">split</span><span class="p">(</span><span class="s">"+"</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="n">sop</span><span class="p">))</span>
    <span class="c1"># 1) Check for carries 
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s">"0"</span><span class="p">,</span> <span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">9</span><span class="p">,</span> <span class="s">"1"</span><span class="p">,</span> <span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="mi">9</span><span class="p">,</span> <span class="s">"&lt;"</span><span class="p">,</span> <span class="s">"0"</span><span class="p">)))</span>
    <span class="c1"># 2) Slide carries to their columns - all in one parallel go!                                         
</span>    <span class="n">gets_carry</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">slide</span><span class="p">(</span><span class="n">gets_carry</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">,</span> <span class="n">gets_carry</span><span class="p">))</span>
    <span class="c1"># 3) Add in carries, and remove overflow from original addition.                                                                                  
</span>    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">gets_carry</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span>
<span class="n">add</span><span class="p">()(</span><span class="s">"683+345"</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/staging/assets/img/2022-12-01-raspy/Blog_99_0.svg" alt="svg"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">683</span> <span class="o">+</span> <span class="mi">345</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1028
</code></pre></div></div> <p>Pretty neat stuff. If you are interested more in this topic, be sure to check at the paper:</p> <p><a href="https://arxiv.org/pdf/2106.06981.pdf">Thinking like Transformers</a> and the <a href="https://github.com/tech-srl/RASP">RASP language</a>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Learn to code as if you were a Transformer.]]></summary></entry></feed>